[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "",
    "text": "The Jack Mackerel Management Strategy Evaluation (MSE) Technical Workshop brought together scientists, technical experts, and external reviewers to review recent progress and refine the MSE framework being developed under SPRFMO. The primary goal of the workshop was to ensure that the modeling framework and management procedures (MPs) are scientifically sound, technically transparent, and aligned with management priorities.\n\n\n\nMSE Framework Consolidation\nParticipants reviewed the jmMSE software package, confirming that it provides a robust and flexible platform for conducting MSEs. The package includes a reference set of operating models conditioned to historical data using MCMC, an efficient MP tuning algorithm, and tools for visualizing and comparing results.\nRobustness Testing\nThe workshop clarified the role and scope of robustness tests. These tests are intended to explore how CMPs perform under a range of plausible yet uncertain scenarios rather than represent definitive alternative models. Scenarios reflecting changes in recruitment, spatial availability, environmental regime shifts (e.g., El Niño), and stock structure were reviewed and refined for implementation.\nIndicator-Driven MPs and HCR logic\nEmpirical MPs based on one or more indicators were evaluated, with focus on two formulations:\n\nTAC as a product of a target and a multiplier from an index.\nTAC adjusted incrementally from the previous year based on index signals.\n\nWe noted that the high current stock status (well within the “green” zone) tended to increase catch levels when tuning to achieve a desired P(Green). This can result in declining stock trends later in the projection period, even when the short-term performance criteria are met.\nRecommendations and Refinements\nThe group recommended additional diagnostics and refinements, including:\n\nAdding plots of how index trajectories relate to TACs.\nIncluding new performance metrics that reflect stock status and trends in the final projection years.\nEnsuring consistent treatment of selectivity, weights-at-age, and catch splits in both projections and reference point calculations.\nExploring robustness scenarios that account for variability in fleet selectivity and biological assumptions, particularly where CPUE is used as an input.\n\nDocumentation and Transparency\nThe group emphasized the importance of transparency in documenting model assumptions, data sources, and MP structure. The group agreed on priorities for improving documentation and sharing annotated examples of MP behavior.\nNext Steps and Implementation\nThe next phase of work will focus on finalizing the candidate MPs, running the robustness tests, and summarizing trade-offs across key performance indicators. In discussions we also identified future reporting needs, including summary tables and figures for managers, and exploration of reference points and evaluation criteria beyond the green zone probability."
  },
  {
    "objectID": "index.html#estimating-relative-availability-from-catch-proportions",
    "href": "index.html#estimating-relative-availability-from-catch-proportions",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Estimating Relative Availability from Catch Proportions",
    "text": "Estimating Relative Availability from Catch Proportions\nTaking an assumption that over a recent period that changes in a smoothed proportion of catch by coastal and offshore areas roughly relates to the effective catchability \\(q\\) (which includes both true catchability and availability) of the fishery, i.e.:\n\\[\n\\text{Catch}_{\\text{fleet}} \\propto q_{\\text{fleet}}\n\\]\nThus, observed catch proportions can serve as a proxy for relative availability.\n\n5-year moving averages of the proportion of the catch occurring in the “coastal” areas compared to the offshore fleet.\n\n\nYear Range\nCoastal (%)\nOffshore (%)\n\n\n\n\n2004–2008\n81\n19\n\n\n2005–2009\n78\n22\n\n\n2006–2010\n74\n26\n\n\n2007–2011\n76\n24\n\n\n2008–2012\n78\n22\n\n\n2009–2013\n82\n18\n\n\n2010–2014\n84\n16\n\n\n2011–2015\n86\n14\n\n\n2012–2016\n86\n14\n\n\n2013–2017\n85\n15\n\n\n2014–2018\n86\n14\n\n\n2015–2019\n87\n13\n\n\n2016–2020\n91\n9\n\n\n2017–2021\n93\n7\n\n\n2018–2022\n94\n6\n\n\n2019–2023\n94\n6\n\n\n2020–2024\n94\n6\n\n\nMean\n85\n15\n\n\n\n\nRange of Change in Estimated Availability\nGiven this pattern we can assume a relative catchability due to an environmental effect. We note that the\n\nCoastal effective catchability increased from a low of 74% (2006–2010) to a high of 94% (2018–2024), a +20 percentage point change.\nOffshore effective catchability declined from 26% to 6%.\n\nAs a sensitivity, we could propose that the effective availability to the offshore fleet drops from 15% of the biomass (the mean) to gradually to 6% during El Niño periods (a 60% decline in \\(q\\) ). This would apply to the data generated for offshore CPUE index in the simulations. For the coastal zones, the effect of El Niño would correspond to an 11% increase in the availability of fish relative to the mean (85%). These changes would apply to the Chilean SC CPUE index and the Peruvian CPUE index data generation. This is one proposal among many that could be imagined. For example a slightly more conservative range could be based on the 10th and 90th percentiles of estimated effective catchability (from proportional catches):\nCoastal:\n•   10th percentile: 77%\n•   90th percentile: 94%\nOffshore:\n•   10th percentile: 6%\n•   90th percentile: 23%\nThese shifts may provide some scope for showing the impact of changes in the relative abundance indicators in index values. These reflect patterns over the past two decades, possibly due to environmental changes."
  },
  {
    "objectID": "index.html#near-term",
    "href": "index.html#near-term",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Near term",
    "text": "Near term\nParticipants were encouraged to document their activities during the workshop, including the methods explored and tuning targets used. Work tasked identified included:\n\nJim evaluated 9 MPs (including bufferdelta2, cpuescore2, test acoustic, and combinations of CPUE indices with different delta_TAC values), all tuned to achieve 60% green status.\nJose/Chile apply shortcut tuning methods to reach similar green zone targets."
  },
  {
    "objectID": "index.html#medium-term",
    "href": "index.html#medium-term",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Medium term",
    "text": "Medium term\n\nSummary Table: MP Methods and Configuration\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nMethod\nMetric\nTuning Parameter\nOther Parameters\nScore Index\nComments\n\n\n\n\n2024+\nbuffer.hcr\ndepletion\ntarget\nbufflow, buffup, limit\ncpuescore3.ind\nOriginal pkg function\n\n\n2024+\nbufferdelta.hcr\ndepletion\nwidth\nsloperatio\ncpuescore3.ind\nModified; not compatible with z-score metrics\n\n\n2024+\nbufferdelta2.hcr\nzscore\nwidth\nsloperatio\ncpuescore2.ind\nNew; not compatible with depletion\n\n\n2024+\nbuffer2.hcr\nzscore\ntarget\nwidth (affects buffer)\ncpuescore2.ind\nOriginal; adjusted for zscore (limit = -2 SD)"
  },
  {
    "objectID": "index.html#day-1-summary",
    "href": "index.html#day-1-summary",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Day 1 summary",
    "text": "Day 1 summary\n\nParticipants and Setup\n\nThe workshop included both in-person and online participation, with representatives from Peru, Chile, Argentina, Ecuador, and the Netherlands, among others.\nThe agenda was described as ambitious, with a focus on hands-on technical work, including software installation and repository access.\n\n\n\nTechnical Infrastructure & Workflow\n\nTwo main GitHub repositories are central: FLjjm (for building FLR objects and running JJM inside the MP) and jjmMSE (the main development site for the MSE work).\nThe workflow follows the DAF (Data, Analysis, Framework) system, with clear steps for data preparation, OEM (Operating Model) conditioning, and performance analysis.\nEmphasis was placed on forking repositories and using branches for collaborative work, with a preference for merging at the end of the week.\nDocker was suggested as a potential solution for ensuring consistent environments across operating systems and participants, though not yet implemented.\n\n\n\nModeling and Simulation\n\nThe group is working with both single-stock and two-stock hypotheses, including a two-stock model with future-applied connectivity based on movement matrices.\nRobustness scenarios are being explored, including cyclic environmental changes and their impacts on productivity.\nThe group discussed the use of “.q” files for efficient storage and handling of large simulation outputs.\n\n\n\nKey Scientific Discussions\n\nThe calculation and interpretation of MSY (Maximum Sustainable Yield) and FMSY were debated, with concerns about the realism of current methods in JJM.\nIt was agreed that the 10-year average of MSY reference points would be used for performance evaluation, to avoid short-term volatility.\nSelectivity patterns and their impact on projections were a major topic. The group considered transitioning from terminal year selectivity to long-term averages over a five-year period to avoid unrealistic jumps in catch projections.\nThere was consensus that the main focus should be on long-term performance of management procedures, but short- and mid-term results are also important for managers.\n\n\n\nEnvironmental and Biological Scenarios\n\nThe workshop addressed the need to model environmental variability, particularly El Niño events, and their impact on stock productivity, weight-at-age, and selectivity.\nLiterature-informed scenarios were presented, with parameters for changes in mortality, recruitment, and spatial distribution.\nRegional differences (e.g., between far north and south stocks) and their implications for catchability and biological responses were discussed.\n\n\n\nAction Items and Next Steps\n\nParticipants will review and potentially refine the environmental scenarios, with a focus on realism and literature support.\nFurther work is planned on selectivity transitions and the technical implementation of gradual changes.\nThe group will continue to test and validate the workflow, with an emphasis on reproducibility and collaborative code development."
  },
  {
    "objectID": "index.html#day-2",
    "href": "index.html#day-2",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Day 2",
    "text": "Day 2\n\nOpening\n\nThe workshop began with participant introductions, including new attendees such as Robert Robinson.\nJim Ianelli welcomed participants, provided a recap of the previous day, and referenced a summary posted in the Teams channel for review.\n\n\n\nReview of Previous Work and Agenda\n\nThe agenda was described as ambitious, with a focus on technical aspects of Management Strategy Evaluation (MSE).\nDiscussion included the effects of El Niño on recruitment and catch distribution, referencing an unsent summary email and a report on the topic.\n\n\n\nTechnical Discussions\n\na. Effects of El Niño and Distribution Shifts\n\nJim presented an analysis of catch distribution changes between coastal and offshore fleets, proposing a 15% average offshore drop (60% decline in Q for offshore fleet) and an 11% increase for coastal zones.\nParticipants debated the appropriateness of using long-term versus short-term averages and the need for smoothing changes rather than step changes.\nIt was agreed that the effect should be applied to both availability and catchability in simulations, with implications for the acoustic survey indices.\n\n\n\nb. Selectivity and Projection Periods\n\nThe group discussed the transition from recent selectivity estimates to long-term means, with concerns about artifacts from using 10-year averages.\nConsensus moved toward using a representative period (2000–2010) for selectivity, avoiding recent years with anomalously high selection for older fish.\nThe need for a smooth transition in selectivity assumptions for projections was emphasized.\n\n\n\nc. Recruitment Regimes and Projections\n\nAnalysis of recruitment means for projections considered the El Niño effect, with a proposed 23–30% bump up in recruitment for recent years.\nDebate ensued on which years to include for calculating means, with a focus on data consistency from 1991 onward.\nThe group discussed the potential for regime shifts and the implications for robustness testing in MSE.\n\n\n\nd. Model Implementation and Coding Practices\n\nDemonstrations were given on the use of R and package structures for running MSE simulations, including best practices for project setup and function sourcing.\nThe importance of consistent selectivity and Q parameter normalization across projections and indices was highlighted.\n\n\n\n\nDecisions and Action Items\n\nAdopt 2000–2010 as the reference period for selectivity in projections, with a smooth transition from current conditions.\nApply a 23–30% recruitment increase for projections reflecting recent El Niño effects, with final years to be confirmed.\nEnsure normalization of selectivity and Q parameters is consistent across all indices and projections.\nContinue refining the codebase, documenting changes, and sharing updates among the technical team.\n\n\n\nOther Business and Closing\n\nParticipants shared experiences with data handling, model setup, and coding challenges.\nThe workshop included informal discussions and technical clarifications.\nThe session concluded with plans to continue reviewing model outputs and performance indicators, and to reconvene as needed for further technical work."
  },
  {
    "objectID": "index.html#day-3",
    "href": "index.html#day-3",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Day 3",
    "text": "Day 3\n\nReview of Technical Issues and Model Adjustments\n\nDiscussion focused on the technical aspects of Management Strategy Evaluation (MSE) for jack mackerel.\nParticipants examined the function and configuration of sliding buffers, control rules, and the impact of selectivity changes.\nThe group reviewed how indices (e.g., CPUE, acoustics) are incorporated, including the use of averages over multiple years and weighting schemes for recent data.\nThere was debate on the variance and correlation structure in observation models, and how these affect simulation results.\n\n\n\nTiming and Implementation of Management Procedures (MPs)\n\nThe workflow and timing for implementing MPs were clarified:\n\nData from 2026 would be used for advice in 2027.\nThe Scientific Committee (SC) would run the MP, with the Commission making final decisions.\nDiscussion on the need for preliminary versus finalized data and the implications for observation error.\n\nThe importance of using the most recent data versus the stability of multi-year averages was highlighted.\n\n\n\nTreatment of Effort Creep and Index Standardization\n\nParticipants agreed that simulating effort creep in future projections is not necessary for the base case, but could be a robustness test.\nThe importance of correcting historical indices for effort creep was emphasized, while future indices are assumed to be unbiased.\nThere was consensus that the standardized or corrected index should be used for both simulation and real-world application.\n\n\n\nSelection and Tuning of Management Procedures\n\nMultiple MPs were tested, each tuned to a 60% probability of meeting the Kobe green zone.\nThe group compared different buffer widths and TAC change limits, analyzing their effects on catch variability and performance metrics.\nDiscussion included the need for clear documentation of specifications and the importance of presenting a set of MPs with distinct trade-offs to the Commission, rather than recommending a single option.\n\n\n\nPerformance Metrics and Projections\n\nThe team reviewed performance across near-term, medium-term, and long-term projections.\nBoxplots and other visualizations were used to compare MPs, focusing on catch variability, probability of stock being in the green zone, and interannual TAC changes.\nConcerns were raised about downward trends in some simulations and the need for additional performance statistics (e.g., probability of stock crash).\n\n\n\nNext Steps and Action Items\n\nAgreement to refine the OM projections and finalize Annex documentation on selectivity and other specifications.\nPlan to continue testing and tuning MPs, with further analysis of performance metrics.\nThe SC will present a set of MPs to the Commission, along with the status quo as a fallback.\nLunch arrangements and informal discussions concluded the session."
  },
  {
    "objectID": "index.html#day-4",
    "href": "index.html#day-4",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Day 4",
    "text": "Day 4\n\nKey Activities and Discussions\n\nModel Runs and Debugging:\n\nOvernight and morning runs were conducted to examine the acoustics index using legacy targets and buffers.\nA significant focus was on debugging the generation of CPUE v3 for 2024 and 2025, investigating unexpected jumps in predicted values.\nAlternative normalization methods for CPUE were tested, following recommendations to improve stability.\n\nAnalysis of Index Jumps:\n\nThe team identified that the jump in the index from 2024 to 2025 was primarily driven by increases in vulnerable biomass and changes in mean weight at age, rather than selectivity changes alone.\nWeighted age calculations and their implications for projections were reviewed in detail, including the use of three-year means and the impact of preliminary data from 2024.\n\nCode Review and Live Demonstration:\n\nLive coding sessions were held to demonstrate how to adjust the OM to exclude problematic years (e.g., dropping 2024 and extending from 2023).\nSmoothing techniques for selectivity and weights at age were discussed and implemented to reduce artificial jumps in projections.\n\nUncertainty and Robustness:\n\nThe group discussed the treatment of process error, residual variability, and autocorrelation in indices.\nEmpirical approaches to setting CVs for indices were compared with default values, and the impact on future projections was considered.\n\nAction Items and Next Steps:\n\nFurther testing of the OM with adjusted years and smoothing is to be completed, with outputs to be pushed under new filenames to avoid disrupting ongoing work.\nContinued analysis of the causes of high catches in test runs and further parameter tuning were assigned.\nThe group agreed to revisit and possibly refine the approach to handling preliminary data and smoothing in both indices and weights at age.\n\n\n\n\nNotable Outcomes\n\nConsensus that both selectivity and mean weight at age contribute to observed index jumps, with smoothing and exclusion of problematic years being viable mitigation strategies.\nAgreement to document and communicate technical progress to the broader group, while maintaining a focus on robust, transparent modeling practices."
  },
  {
    "objectID": "index.html#day-5",
    "href": "index.html#day-5",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Day 5",
    "text": "Day 5\n\nModel Runs and Technical Issues\n\nThe group reviewed progress on running various management procedures (MPs), focusing on the acoustic and CPUE indices. Issues with FL libraries and model reproducibility were discussed, with fixes applied to ensure models ran as expected.\nRobustness tests were conducted, particularly comparing the performance of different indices (acoustic, CPUE3, CPUE6, 3.6). The need to clarify how “shortcut” procedures treat stocks was debated, especially regarding biomass tracking and catch splits.\nThe group noted that tuning parameters (e.g., width, slope ratio) and their impact on model performance remain a challenge, especially when standardizing across indices.\n\n\n\nInterpretation and Presentation of Results\n\nThere was significant discussion about interpreting outputs, particularly when catch trends did not align with biomass trends. Concerns were raised about the credibility of certain indices and the need for clearer communication of model behavior.\nThe importance of visualizing trade-offs and the response of TAC to indices was emphasized, with ongoing efforts to develop summary figures for inclusion in reports.\n\n\n\nWorkflows, Code, and Collaboration\n\nParticipants shared experiences with the codebase, noting progress in understanding and modifying functions, but also highlighting the need for further documentation and standardization.\nThe group agreed on the value of reproducibility and transparency, with suggestions to document daily progress and maintain clear records for future reference.\n\n\n\nPlanning and Next Steps\n\nThe group recognized that while technical progress was made, the process is ongoing. There was consensus on the need for another technical workshop (ideally in person) to continue development and evaluation of MPs.\nThe timeline for delivering a report to the Scientific Committee (SC) and Commission was discussed, with acknowledgment that final recommendations are not yet possible. Instead, the report will focus on documenting progress, challenges, and a proposed work plan for the coming year.\nConcerns about funding, continuity (especially regarding software and contracts), and the need for member commitment to ongoing tool development were raised.\n\n\n\nRecommendations and Reflections\n\nThe group agreed to recommend continued development and evaluation of MPs, with an emphasis on transparency, reproducibility, and clear communication to managers.\nIt was noted that, if a new MP is not ready for 2026, the existing method (Annex K) will remain in use.\nThere was broad recognition of the complexity and time required for this process, and appreciation for the collaborative progress made during the workshop."
  },
  {
    "objectID": "index.html#defining-management-procedures-using-mpctrl-and-msectrl",
    "href": "index.html#defining-management-procedures-using-mpctrl-and-msectrl",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Defining Management Procedures using mpCtrl and mseCtrl",
    "text": "Defining Management Procedures using mpCtrl and mseCtrl\nIn the mse package, Management Procedures (MPs) are constructed as modular sequences of functional components that simulate how a fishery would be managed under alternative strategies. These strategies are defined using mpCtrl, which organizes component modules defined via mseCtrl.\nThis modular design allows you to:\n\nSelect estimation methods for stock status (est)\nDefine harvest control rules (hcr, phcr)\nSimulate implementation systems (isys)\nInclude optional technical measures (tm)\n\n\nStructure of mpCtrl\nThe mpCtrl() constructor takes a named list of components. Each component must be an mseCtrl object that defines the function to use (method) and its input parameters (args).\n\n\nShow code\nctrl &lt;- mpCtrl(list(\n  est = mseCtrl(method = shortcut.sa, args = list(\n    metric = \"depletion\",\n    devs = metdevs,\n    B0 = refpts(om)$SB0\n  )),\n  hcr = mseCtrl(method = buffer.hcr, args = list(\n    target  = 1000,\n    bufflow = 0.30,\n    buffupp = 0.50,\n    lim     = 0.10,\n    min     = 0,\n    metric  = \"depletion\"\n  )),\n  isys = mseCtrl(method = split.is, args = list(\n    split = catch_props(om)$last5\n  ))\n))\n\n\n\n\nExplanation of Components\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nest\nThe estimator module determines how the stock status is assessed during each management cycle.\n\n\nhcr\nThe harvest control rule uses the estimated status to recommend a total allowable catch or effort level.\n\n\nphcr\n(Optional) Parametrization helper for complex or adaptive control rules.\n\n\nisys\nSimulates how the recommended catch is implemented across fleets, often using historical proportions or allocation logic.\n\n\ntm\n(Optional) Includes non-catch-based rules, such as minimum size limits or gear restrictions.\n\n\n\n\n\nCommon method and args Options\n\n\n\n\n\n\n\n\nComponent\nExample method\nCommon args\n\n\n\n\nest\nperfect.sa, shortcut.sa\nmetric, devs, B0, years\n\n\nhcr\nbuffer.hcr, hockeystick.hcr, trend.hcr\ntarget, lim, bufflow, buffupp, trigger, metric\n\n\nisys\nsplit.is, fixed.is\nsplit, noise, bias\n\n\ntm\nuser-defined\nsize or spatial constraints\n\n\n\n\n\nExample: Building an mseCtrl for a Harvest Control Rule\nEach mseCtrl specifies a method function and its arguments.\n\n\nShow code\nctl &lt;- mseCtrl(\n  method = buffer.hcr,\n  args = list(\n    target  = 1000,\n    bufflow = 0.30,\n    buffupp = 0.50,\n    lim     = 0.10,\n    min     = 0,\n    metric  = \"depletion\"\n  )\n)\n\n\n\n\nAccessor Functions\nYou can programmatically access or modify components of an mpCtrl object:\n\n\nShow code\n# Access or change the HCR method\nmethod(ctrl@hcr)\n\n# Update the target biomass value\nargs(ctrl@hcr)$target &lt;- 1200\n\n\nAlternatively, use accessor functions:\n\n\nShow code\nhcr(ctrl) &lt;- mseCtrl(method = new.hcr, args = list(...))\nargs(ctrl, \"hcr\")$target &lt;- 1100\n\n\n\n\n\nSummary\nThis framework enables:\n\nRapid prototyping of management strategies\nTransparent comparisons across MPs\nFlexible integration with simulated operating models\n\nBy separating each component of an MP into a function-object pair, the mse package supports reproducible, configurable, and extensible MSE design workflows.\nTo analyze the behavior of bufferdelta.hcr() over the range of index values used as input (i.e., the stock status metric like “depletion” or “zscore”), the key output to examine is the harvest control multiplier hcrm—which determines how much the TAC is adjusted relative to the previous TAC. This multiplier is a piecewise function of the index value at the data year.\n\n\nHarvest Control Rule (HCR) with Buffer Delta\nThis HCR formulation uses a smoothed transition based on a buffer zone around a biomass or metric target. The response scalar \\(h(m)\\), applied to the previous catch, is defined based on the relative metric value \\(m\\) (e.g., standardized index or depletion level), and follows a piecewise logic:\nLet:\n\n\\(m\\): observed metric (e.g., index value)\n\\(t\\): target level\n\\(w\\): buffer width\n\\(l = t - 2w\\): limit threshold\n\\(b_{\\text{low}} = t - w\\): buffer lower bound\n\\(b_{\\text{upp}} = t + w\\): buffer upper bound\n\\(r\\): slope ratio\n\nThen the Harvest Control Rule (HCR) response multiplier \\(h(m)\\) is:\n\\[\nh(m) =\n\\begin{cases}\n\\frac{1}{2} \\left(\\frac{m}{l}\\right)^2, & \\text{if } m \\leq l \\\\\n\\frac{1}{2} \\left(1 + \\frac{m - l}{b_{\\text{low}} - l} \\right), & \\text{if } l &lt; m &lt; b_{\\text{low}} \\\\\n1, & \\text{if } b_{\\text{low}} \\leq m &lt; b_{\\text{upp}} \\\\\n1 + r \\cdot \\frac{1}{2(b_{\\text{low}} - l)} (m - b_{\\text{upp}}), & \\text{if } m \\geq b_{\\text{upp}} \\\\\n\\end{cases}\n\\]\nThe resulting Total Allowable Catch (TAC) is calculated as:\n\\[\n\\text{TAC}_{\\text{new}} = \\text{TAC}_{\\text{previous}} \\cdot h(m)\n\\]\nOptional constraints on TAC changes can be applied:\n\\[\n\\text{TAC}_{\\text{new}} = \\min(\\text{TAC}_{\\text{new}}, \\text{TAC}_{\\text{previous}} \\cdot d_{\\text{upp}})\n\\]\n\\[\n\\text{TAC}_{\\text{new}} = \\max(\\text{TAC}_{\\text{new}}, \\text{TAC}_{\\text{previous}} \\cdot d_{\\text{low}})\n\\]\n\n\n🔁 Behavior Summary\nLet’s denote:\n\\(m\\): index metric (e.g., depletion) at the data year\n\\(\\text{target}\\): central value, e.g., 0.5\n\\(\\text{width}\\): buffer width, e.g., 1.0\nThen:\n\\(\\text{bufflow} = \\text{target} - \\text{width}\\)\n\\(\\text{buffupp} = \\text{target} + \\text{width}\\)\n\\(\\text{lim} = \\text{target} - 2 \\cdot \\text{width}\\)\n\n🧭 HCR behavior across index values\n\n\n\n\n\n\n\n\n\nIndex value \\(m\\)\nDescription\nMultiplier h(m)\nTAC behavior\n\n\n\n\n\\(m \\leq \\text{lim}\\)\nVery low index\nQuadratic increase from 0 → 0.5\nStrong reduction\n\n\n\\(\\text{lim} &lt; m &lt; \\text{bufflow}\\)\nBetween limit and lower buffer\nLinear rise from 0.5 to 1\nModerate reduction\n\n\n\\(\\text{bufflow} \\leq m &lt; \\text{buffupp}\\)\nWithin buffer zone\nFlat at 1\nNo change\n\n\n\\(m \\geq \\text{buffupp}\\)\nAbove upper buffer\nLinear increase starting at 1 (slope = sloperatio)\nModerate increase\n\n\n\n\n\n🔎 Example with Default Parameters\nIf you use the defaults:\ntarget = 0.5\nwidth = 1\nsloperatio = 0.2\nThen:\n•   bufflow = -0.5, buffupp = 1.5, lim = -1.5\n\n•   The flat zone is from -0.5 to 1.5 (note: with depletion metric, this wide range makes sense for standardized metrics like z-scores, but not raw depletion)\nIf using depletion as the metric, you’d typically want:\ntarget = 0.4\nwidth = 0.1\nsloperatio = 0.2\n→ lim = 0.2, bufflow = 0.3, buffupp = 0.5 So the response curve looks like:\n\n\nShow code\nplot_hcrm &lt;- function(target = 0.4, width = 0.1, sloperatio = 0.2, metric_range = seq(0, 1, 0.01)) {\n  bufflow &lt;- target - width\n  buffupp &lt;- target + width\n  lim &lt;- target - 2 * width\n\n  hcrm &lt;- ifelse(metric_range &lt;= lim,\n    ((metric_range / lim)^2) / 2,\n    ifelse(metric_range &lt; bufflow,\n      0.5 * (1 + (metric_range - lim) / (bufflow - lim)),\n      ifelse(metric_range &lt; buffupp,\n        1,\n        1 + sloperatio * 1 / (2 * (bufflow - lim)) * (metric_range - buffupp)\n      )\n    )\n  )\n\n  plot(metric_range, hcrm,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Metric (e.g., depletion)\", ylab = \"Harvest multiplier (hcrm)\",\n    main = \"Response of TAC to Index Metric\"\n  )\n  abline(h = 1, col = \"gray\", lty = 2)\n  abline(v = c(lim, bufflow, buffupp), col = \"red\", lty = 3)\n}\nplot_hcrm(target = 0.4, width = 0.1, sloperatio = 0.2)\n\n\n\n\n\n\n\n\n\nThis shows the piecewise nature of the multiplier and can be tailored to any input metric (depletion, zscore, etc.)."
  },
  {
    "objectID": "index.html#overview-of-cpuescore",
    "href": "index.html#overview-of-cpuescore",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Overview of cpuescore",
    "text": "Overview of cpuescore\nIn the jmMSE framework, different CPUE scoring functions are used to inform harvest control rules (HCRs). These functions standardize or compare CPUE time series across simulations and reference periods. The three primary scoring methods are:\n\ncpuescore.z\ncpuescore.mean\ncpuescore.level"
  },
  {
    "objectID": "index.html#z-score-standardization-cpuescore.z",
    "href": "index.html#z-score-standardization-cpuescore.z",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "1. Z-score Standardization: cpuescore.z",
    "text": "1. Z-score Standardization: cpuescore.z\nThis method standardizes the CPUE values by subtracting the mean and dividing by the standard deviation across simulations:\n\\[\n\\text{score}_{i,t} = \\frac{\\text{CPUE}_{i,t} - \\mu_t}{\\sigma_t}\n\\]\nWhere:\n\\(\\mu_t\\) = mean CPUE in year \\(t\\) across simulations\n\\(\\sigma_t\\) = standard deviation in year \\(t\\) across simulations\nThis produces a unitless, centered score:\n\n\nShow code\nscore &lt;- (met[, ac(dy[i])] %-% yearMeans(ref)) %/% sqrt(yearVars(ref))\n\n\nUseful when you want to assess relative anomalies in CPUE from expected trends.\n⸻"
  },
  {
    "objectID": "index.html#mean-ratio-cpuescore.mean",
    "href": "index.html#mean-ratio-cpuescore.mean",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "2. Mean Ratio: cpuescore.mean",
    "text": "2. Mean Ratio: cpuescore.mean\nThis method compares the mean CPUE in recent years (dy) to a reference mean CPUE: \\[\n\\text{score}_i = \\frac{\\bar{\\text{CPUE}}_{dy, i}}{\\bar{\\text{CPUE}}_{ref, i}}\n\\] This is a relative index level and is not standardized:\n\n\nShow code\nscore &lt;- yearMeans(met[, ac(dy[i])]) %/% yearMeans(ref)\n\n\nOften used when absolute differences in mean CPUE should affect TAC decisions."
  },
  {
    "objectID": "index.html#raw-index-cpuescore.level",
    "href": "index.html#raw-index-cpuescore.level",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "3. Raw Index: cpuescore.level",
    "text": "3. Raw Index: cpuescore.level\nThis method passes through the CPUE values with no transformation:\n\\[\n\\text{score}_{i,t} = \\text{CPUE}_{i,t}\n\\]\n\n\nShow code\nscore &lt;- met[, ac(dy[i])]\n\n\nUsed when raw or smoothed CPUE indices are deemed directly interpretable and comparable."
  },
  {
    "objectID": "index.html#example-simulated-scores",
    "href": "index.html#example-simulated-scores",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Example: Simulated Scores",
    "text": "Example: Simulated Scores\nThe following example shows how the three cpuescore methods behave using simulated CPUE data across 100 simulations and 10 years.\n\n\nShow code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Simulate CPUE data\nset.seed(42)\nn_sim &lt;- 100\nn_year &lt;- 10\nind_sim &lt;- matrix(rlnorm(n_sim * n_year, meanlog = log(1), sdlog = 0.2), nrow = n_sim)\n\n# Reference: all years; \"recent\" period: years 9-10\nref_mean &lt;- rowMeans(ind_sim)\nref_sd &lt;- apply(ind_sim, 1, sd)\n\nz_scores &lt;- (ind_sim[, 10] - ref_mean) / ref_sd\nmean_scores &lt;- rowMeans(ind_sim[, 9:10]) / ref_mean\nraw_scores &lt;- ind_sim[, 10]\n\n# Reshape for plotting\nscore_df &lt;- tibble(\n  sim = 1:n_sim,\n  z = z_scores,\n  mean = mean_scores,\n  level = raw_scores\n) %&gt;% pivot_longer(cols = -sim, names_to = \"score_type\", values_to = \"score\")\n\nggplot(score_df, aes(x = sim, y = score)) +\n  geom_line() +\n  # facet_wrap(~score_type, scales = \"free_y\") +\n  facet_wrap(~score_type) +\n  labs(\n    title = \"Simulated CPUE Score Types\",\n    x = \"Simulation\",\n    y = \"Score Value\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "index.html#summary-1",
    "href": "index.html#summary-1",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Summary",
    "text": "Summary\nThe indices were refactored from the cpuescore functions to avoid confusion with surveys and can be summarized as follows:\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nNormalized\nUse case\n\n\n\n\nindscore.z\nStandardize vs mean/sd\n✅\nCompare anomalies across sims\n\n\nindscore.mean\nMean ratio of dy vs ref\n❌\nIndex levels matter\n\n\nindscore.level\nRaw CPUE values used as-is\n❌\nWhen index is well-calibrated\n\n\n\n⸻\n\n\nShow code\nindscore.z &lt;- function(\n    stk, idx, index = 1, dlag = rep(args$data_lag, length(index)),\n    refyrs = NULL, args, tracking) {\n  dlag &lt;- setNames(dlag, nm = names(idx)[index])\n  ay &lt;- args$ay\n  dy &lt;- ay - dlag\n  res &lt;- as.list(setNames(nm = names(idx)[index]))\n\n  for (i in names(res)) {\n    met &lt;- window(idx[[i]], end = dy[i]) # removed biomass()\n\n    ref &lt;- if (!is.null(refyrs)) met[, ac(refyrs)] else met\n\n    res[[i]] &lt;- (met[, ac(dy[i])] %-% yearMeans(ref)) %/% sqrt(yearVars(ref))\n\n    dimnames(res[[i]])$year &lt;- max(dy)\n\n    track(tracking, paste0(\"score.mean.\", i), ac(ay)) &lt;- yearMeans(ref)\n    track(tracking, paste0(\"score.sd.\", i), ac(ay)) &lt;- sqrt(yearVars(ref))\n    track(tracking, paste0(\"score.ind.\", i), ac(ay)) &lt;- res[[i]]\n  }\n\n  return(list(stk = stk, ind = res, tracking = tracking, cpue = met))\n}\nbufferdelta.hcr &lt;- function(stk, ind, target = 0.5, width = 1,\n                            sloperatio = 0.2, dupp = NULL, dlow = NULL,\n                            args, tracking) {\n  # setup\n  ay &lt;- args$ay\n  iy &lt;- args$iy\n  frq &lt;- args$frq\n  man_lag &lt;- args$management_lag\n  cys &lt;- seq(ay + man_lag, ay + man_lag + frq - 1)\n\n  # compute score\n  score &lt;- ind[[1]] # assuming single FLQuant named 'zscore', 'mean_ratio', or 'level'\n\n  # define slope\n  bufflow &lt;- target - width\n  buffupp &lt;- target + width\n  lim &lt;- target - 2 * width\n  slope &lt;- sloperatio / width\n\n  # compute h(score)\n  h &lt;- ifelse(score &lt; lim, 0,\n    ifelse(score &lt; bufflow,\n      slope * (score - lim),\n      ifelse(score &gt; buffupp, 1 + sloperatio * (score - buffupp), 1)\n    )\n  )\n\n  tac_prev &lt;- catch(stk)[, ac(iy - 1)]\n  tac_new &lt;- tac_prev * h\n\n  # apply TAC limits if given\n  if (!is.null(dupp)) tac_new &lt;- pmin(tac_new, tac_prev * dupp)\n  if (!is.null(dlow)) tac_new &lt;- pmax(tac_new, tac_prev * dlow)\n\n  catch(stk)[, ac(cys)] &lt;- tac_new\n  return(list(stk = stk, ind = ind, tracking = tracking))\n}\n\nargs &lt;- list(ay = 2025, iy = 2026, frq = 1, data_lag = 1, management_lag = 1)\nidx &lt;- list(\"cpue\" = FLQuant(rlnorm(30), dimnames = list(year = 1996:2025)))\n\nstk &lt;- FLStock(catch = FLQuant(1000, dimnames = list(year = 2025:2030)))\n\nresult &lt;- indscore.z(stk, idx, args = args, tracking = FLQuant(0))\noutput &lt;- bufferdelta.hcr(result$stk, result$ind, args = args, tracking = result$tracking)\n\ncatch(output$stk)\n\nargs &lt;- list(ay = 2025, iy = 2026, frq = 1, data_lag = 1, management_lag = 1)\nidx &lt;- list(\"cpue\" = FLQuant(rlnorm(30), dimnames = list(year = 1996:2025)))\n\nresult &lt;- indscore.z(stk, idx, args = args, tracking = FLQuant(0))\noutput &lt;- bufferdelta.hcr(result$stk, result$ind, args = args, tracking = result$tracking)"
  },
  {
    "objectID": "index.html#environment-objects",
    "href": "index.html#environment-objects",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Environment Objects",
    "text": "Environment Objects\n\n\n\n\n\n\n\nObject Name\nType / Purpose\n\n\n\n\nh1\nA list containing the full OM, OEM, and IEM for hypothesis H1 (qs file)\n\n\nom\nIterated subset of the Operating Model from h1\n\n\noem, iem\nObservation and implementation error models; extracted from h1\n\n\nomperf\nPerformance metrics of OM alone, usually C, F, SB for conditioning years\n\n\nperf\nCombined data frame of MP simulation performance results\n\n\ngetSlick\nFunction that merges MP/OM results and constructs a Slick summary object\n\n\nFLslick\nConstructor function that builds and returns a Slick object for plotting\n\n\nsli\nThe returned Slick object for visualization (Kobe, Quilt, Spider, etc.)\n\n\nctrl\nA list of control parameters for MPs (e.g., estimation methods, tuning devs)\n\n\ncondition\nNot found in current project files; possibly a misidentified object"
  },
  {
    "objectID": "index.html#helper-functions-to-plot-results",
    "href": "index.html#helper-functions-to-plot-results",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Helper Functions to Plot Results",
    "text": "Helper Functions to Plot Results\n\n\nShow code\nplot_slick_quilt(sli, stat = \"longterm C\")\nplot_slick_spider(sli, om_idx = 1, mp_idx = 2)\nplot_slick_tradeoff(sli, stat = \"PSBMSY\")"
  },
  {
    "objectID": "index.html#defining-management-procedures-using-mpctrl-and-msectrl-1",
    "href": "index.html#defining-management-procedures-using-mpctrl-and-msectrl-1",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Defining Management Procedures using mpCtrl and mseCtrl",
    "text": "Defining Management Procedures using mpCtrl and mseCtrl\nIn the mse package, Management Procedures (MPs) are modularly constructed using the mpCtrl class, which bundles together component controls (mseCtrl) for estimation, decision-making, and implementation.\n\nStructure of mpCtrl\n\n\nComponents and Options\n\n\n\n\n\n\n\n\n\nComponent\nDescription\nExample method\nTypical args\n\n\n\n\nest\nEstimator for stock status\nperfect.sa, shortcut.sa\nmetric, B0, devs, years\n\n\nhcr\nHarvest Control Rule\nbuffer.hcr, hockeystick.hcr, trend.hcr\ntarget, lim, trigger, metric, buffer\n\n\nphcr\nParametrization of HCR (optional)\nparametric.hcr, etc.\nCustom parameters\n\n\nisys\nImplementation system (e.g., allocation strategy)\nsplit.is\nsplit = catch_props(...), noise, bias\n\n\ntm\nTechnical measures (e.g., size limits, gear rules)\n–\nOptional, rarely used in CJM MSE setup\n\n\n\n\n\nSummary of mseCtrl\nAn mseCtrl object specifies how a module runs:\n\nmethod: a function to apply (e.g., shortcut.sa)\nargs: list of arguments passed to the method\n\n\nExample\n\n\nShow code\nmseCtrl(\n  method = buffer.hcr,\n  args = list(\n    target  = 1000,\n    bufflow = 0.30,\n    buffupp = 0.50,\n    lim     = 0.10,\n    min     = 0,\n    metric  = \"depletion\"\n  )\n)\n\n\n\n\n\nAccessors and Replacements\nYou can modify or retrieve components using:\n\n\nShow code\nmethod(ctrl@hcr)\nargs(ctrl@hcr)$target &lt;- 1200\n\n\nOr, using provided accessors:\n\n\nShow code\nhcr(ctrl) &lt;- mseCtrl(method = new.hcr, args = list(...))\nargs(ctrl, \"hcr\")$target &lt;- 1100\n\n\nThis modular framework allows for flexible and transparent testing of MPs within the MSE simulation, including full customization of estimation, control rules, and implementation behavior. This notebook supports the JM MSE development process and is intended for use during scenario comparison, workshop reporting, and trade-off evaluation."
  },
  {
    "objectID": "index.html#slick-object-structure",
    "href": "index.html#slick-object-structure",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Slick Object Structure",
    "text": "Slick Object Structure\nThe Slick object is the core summary container created by the getSlick() and FLslick() functions. It contains performance data used for visualization and evaluation across multiple Management Procedures (MPs) and Operating Models (OMs).\n\n\n\nSlot\nContents\n\n\n\n\n@Boxplot\nMP × OM × performance indicators (boxplots)\n\n\n@Kobe\nSB/SBMSY vs F/FMSY over kobeyrs\n\n\n@Quilt\nHeatmap of average performance\n\n\n@Spider\nScaled performance for visual trade-offs\n\n\n@Timeseries\nTime series of F, C, SB\n\n\n@Tradeoff\nMean trade-off indicators (post-OM years)\n\n\n@MPs, @OMs\nMetadata: MP and OM definitions and labels"
  },
  {
    "objectID": "index.html#creating-and-visualizing-a-slick-object",
    "href": "index.html#creating-and-visualizing-a-slick-object",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Creating and Visualizing a Slick Object",
    "text": "Creating and Visualizing a Slick Object\n\n\nShow code\n# Load OM and compute baseline performance\nh1 &lt;- qread(\"data/h1_1.07.qs\")\nom &lt;- iter(h1$om, seq(100))\nomperf &lt;- performance(om, years = 1970:2023, statistics = statistics[c(\"C\", \"F\", \"SB\")])\n\nperf &lt;- readPerformance(\"demo/performance.dat.gz\")\n\n# Combine with MP results (perf), filtering to \"tune\" runs\nsli &lt;- getSlick(perf[grep(\"tune\", run)], omperf, kobeyrs = 2034:2042)"
  },
  {
    "objectID": "index.html#optional-shorten-mp-labels-for-plotting",
    "href": "index.html#optional-shorten-mp-labels-for-plotting",
    "title": "Report of the MSE workshop, July 14-18, 2025",
    "section": "Optional: Shorten MP Labels for Plotting",
    "text": "Optional: Shorten MP Labels for Plotting\n\n\nShow code\nshorten_mp &lt;- function(mpnames) {\n  gsub(\"h1_1.07_\", \"\", mpnames) |&gt;\n    gsub(\"cpue2\", \"C2\", .) |&gt;\n    gsub(\"cpue36\", \"C36\", .) |&gt;\n    gsub(\"cpue3\", \"C3\", .) |&gt;\n    gsub(\"scbad\", \"SCB\", .) |&gt;\n    gsub(\"scgood\", \"SCG\", .) |&gt;\n    gsub(\"scmedium\", \"SCM\", .) |&gt;\n    gsub(\"buffer\", \"BUF\", .) |&gt;\n    gsub(\"hcr_target_\", \"HCR_T\", .) |&gt;\n    gsub(\"tune\", \"T\", .)\n}\nperf[, mp := paste0(\"H1_\", shorten_mp(mp))]"
  }
]