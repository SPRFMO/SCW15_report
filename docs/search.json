[
  {
    "objectID": "doc/App_E.html",
    "href": "doc/App_E.html",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "This document is a collection of notes on the structure and behavior of key objects used in the jmMSE Management Strategy Evaluation (MSE) framework for SPRFMO Jack Mackerel. It documents the modeling components (h1, om, perf), performance calculations, and the use of getSlick() and FLslick() to generate evaluation plots.\nThis modular framework allows for flexible and transparent testing of MPs within the MSE simulation, including full customization of estimation, control rules, and implementation behavior. This notebook supports the JM MSE development process and is intended for use during scenario comparison, workshop reporting, and trade-off evaluation.\n\n\nIn the mse package, Management Procedures (MPs) are constructed as modular sequences of functional components that simulate how a fishery would be managed under alternative strategies. These strategies are defined using mpCtrl, which organizes component modules defined via mseCtrl.\nThis modular design allows you to:\n\nSelect estimation methods for stock status (est)\nDefine harvest control rules (hcr, phcr)\nSimulate implementation systems (isys)\nInclude optional technical measures (tm)\n\n\n\nThe mpCtrl() constructor takes a named list of components. Each component must be an mseCtrl object that defines the function to use (method) and its input parameters (args).\n\n\nShow the code\nctrl &lt;- mpCtrl(list(\n  est = mseCtrl(method = shortcut.sa, args = list(\n    metric = \"depletion\",\n    devs = metdevs,\n    B0 = refpts(om)$SB0\n  )),\n  hcr = mseCtrl(method = buffer.hcr, args = list(\n    target  = 1000,\n    bufflow = 0.30,\n    buffupp = 0.50,\n    lim     = 0.10,\n    min     = 0,\n    metric  = \"depletion\"\n  )),\n  isys = mseCtrl(method = split.is, args = list(\n    split = catch_props(om)$last5\n  ))\n))\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nest\nThe estimator module determines how the stock status is assessed during each management cycle.\n\n\nhcr\nThe harvest control rule uses the estimated status to recommend a total allowable catch or effort level.\n\n\nphcr\n(Optional) Parametrization helper for complex or adaptive control rules.\n\n\nisys\nSimulates how the recommended catch is implemented across fleets, often using historical proportions or allocation logic.\n\n\ntm\n(Optional) Includes non-catch-based rules, such as minimum size limits or gear restrictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\nExample method\nCommon args\n\n\n\n\nest\nperfect.sa, shortcut.sa\nmetric, devs, B0, years\n\n\nhcr\nbuffer.hcr, hockeystick.hcr, trend.hcr\ntarget, lim, bufflow, buffupp, trigger, metric\n\n\nisys\nsplit.is, fixed.is\nsplit, noise, bias\n\n\ntm\nuser-defined\nsize or spatial constraints\n\n\n\n\n\n\nEach mseCtrl specifies a method function and its arguments.\n\n\nShow the code\nctl &lt;- mseCtrl(\n  method = buffer.hcr,\n  args = list(\n    target  = 1000,\n    bufflow = 0.30,\n    buffupp = 0.50,\n    lim     = 0.10,\n    min     = 0,\n    metric  = \"depletion\"\n  )\n)\n\n\n\n\n\nYou can programmatically access or modify components of an mpCtrl object:\n\n\nShow the code\n# Access or change the HCR method\nmethod(ctrl@hcr)\n\n# Update the target biomass value\nargs(ctrl@hcr)$target &lt;- 1200\n\n\nAlternatively, use accessor functions:\n\n\nShow the code\nhcr(ctrl) &lt;- mseCtrl(method = new.hcr, args = list(...))\nargs(ctrl, \"hcr\")$target &lt;- 1100\n\n\n\n\n\n\nThis framework enables:\n\nRapid prototyping of management strategies\nTransparent comparisons across MPs\nFlexible integration with simulated operating models\n\nBy separating each component of an MP into a function-object pair, the mse package supports reproducible, configurable, and extensible MSE design workflows.\nTo analyze the behavior of bufferdelta.hcr() over the range of index values used as input (i.e., the stock status metric like “depletion” or “zscore”), the key output to examine is the harvest control multiplier hcrm—which determines how much the TAC is adjusted relative to the previous TAC. This multiplier is a piecewise function of the index value at the data year.\n\n\n\nThis HCR formulation uses a smoothed transition based on a buffer zone around a biomass or metric target. The response scalar \\(h(m)\\), applied to the previous catch, is defined based on the relative metric value \\(m\\) (e.g., standardized index or depletion level), and follows a piecewise logic:\nLet:\n\n\\(m\\): observed metric (e.g., index value)\n\\(t\\): target level\n\\(w\\): buffer width\n\\(l = t - 2w\\): limit threshold\n\\(b_{\\text{low}} = t - w\\): buffer lower bound\n\\(b_{\\text{upp}} = t + w\\): buffer upper bound\n\\(r\\): slope ratio\n\nThen the Harvest Control Rule (HCR) response multiplier \\(h(m)\\) is:\n\\[\nh(m) =\n\\begin{cases}\n\\frac{1}{2} \\left(\\frac{m}{l}\\right)^2, & \\text{if } m \\leq l \\\\\n\\frac{1}{2} \\left(1 + \\frac{m - l}{b_{\\text{low}} - l} \\right), & \\text{if } l &lt; m &lt; b_{\\text{low}} \\\\\n1, & \\text{if } b_{\\text{low}} \\leq m &lt; b_{\\text{upp}} \\\\\n1 + r \\cdot \\frac{1}{2(b_{\\text{low}} - l)} (m - b_{\\text{upp}}), & \\text{if } m \\geq b_{\\text{upp}} \\\\\n\\end{cases}\n\\]\nThe resulting Total Allowable Catch (TAC) is calculated as:\n\\[\n\\text{TAC}_{\\text{new}} = \\text{TAC}_{\\text{previous}} \\cdot h(m)\n\\]\nOptional constraints on TAC changes can be applied:\n\\[\n\\text{TAC}_{\\text{new}} = \\min(\\text{TAC}_{\\text{new}}, \\text{TAC}_{\\text{previous}} \\cdot d_{\\text{upp}})\n\\]\n\\[\n\\text{TAC}_{\\text{new}} = \\max(\\text{TAC}_{\\text{new}}, \\text{TAC}_{\\text{previous}} \\cdot d_{\\text{low}})\n\\]\n\n\n\nLet’s denote:\n\\(m\\): index metric (e.g., depletion) at the data year\n\\(\\text{target}\\): central value, e.g., 0.5\n\\(\\text{width}\\): buffer width, e.g., 1.0\nThen:\n\\(\\text{bufflow} = \\text{target} - \\text{width}\\)\n\\(\\text{buffupp} = \\text{target} + \\text{width}\\)\n\\(\\text{lim} = \\text{target} - 2 \\cdot \\text{width}\\)\n\n\n\n\n\n\n\n\n\n\n\nIndex value \\(m\\)\nDescription\nMultiplier h(m)\nTAC behavior\n\n\n\n\n\\(m \\leq \\text{lim}\\)\nVery low index\nQuadratic increase from 0 → 0.5\nStrong reduction\n\n\n\\(\\text{lim} &lt; m &lt; \\text{bufflow}\\)\nBetween limit and lower buffer\nLinear rise from 0.5 to 1\nModerate reduction\n\n\n\\(\\text{bufflow} \\leq m &lt; \\text{buffupp}\\)\nWithin buffer zone\nFlat at 1\nNo change\n\n\n\\(m \\geq \\text{buffupp}\\)\nAbove upper buffer\nLinear increase starting at 1 (slope = sloperatio)\nModerate increase\n\n\n\n\n\n\nIf you use the defaults:\ntarget = 0.5\nwidth = 1\nsloperatio = 0.2\nThen:\n•   bufflow = -0.5, buffupp = 1.5, lim = -1.5\n\n•   The flat zone is from -0.5 to 1.5 (note: with depletion metric, this wide range makes sense for standardized metrics like z-scores, but not raw depletion)\nIf using depletion as the metric, you’d typically want:\ntarget = 0.4\nwidth = 0.1\nsloperatio = 0.2\n→ lim = 0.2, bufflow = 0.3, buffupp = 0.5 So the response curve looks like:\n\n\nShow the code\nplot_hcrm &lt;- function(target = 0.4, width = 0.1, sloperatio = 0.2, metric_range = seq(0, 1, 0.01)) {\n  bufflow &lt;- target - width\n  buffupp &lt;- target + width\n  lim &lt;- target - 2 * width\n\n  hcrm &lt;- ifelse(metric_range &lt;= lim,\n    ((metric_range / lim)^2) / 2,\n    ifelse(metric_range &lt; bufflow,\n      0.5 * (1 + (metric_range - lim) / (bufflow - lim)),\n      ifelse(metric_range &lt; buffupp,\n        1,\n        1 + sloperatio * 1 / (2 * (bufflow - lim)) * (metric_range - buffupp)\n      )\n    )\n  )\n\n  plot(metric_range, hcrm,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Metric (e.g., depletion)\", ylab = \"Harvest multiplier (hcrm)\",\n    main = \"Response of TAC to Index Metric\"\n  )\n  abline(h = 1, col = \"gray\", lty = 2)\n  abline(v = c(lim, bufflow, buffupp), col = \"red\", lty = 3)\n}\nplot_hcrm(target = 0.4, width = 0.1, sloperatio = 0.2)\n\n\n\n\n\n\n\n\n\nThis shows the piecewise nature of the multiplier and can be tailored to any input metric (depletion, zscore, etc.).\n\n\n\n\n\nIn the jmMSE framework, different CPUE scoring functions are used to inform harvest control rules (HCRs). These functions standardize or compare CPUE time series across simulations and reference periods. The three primary scoring methods are:\n\ncpuescore.z\ncpuescore.mean\ncpuescore.level\n\nThese names were modified from the original cpuescore functions to provide more clarity. Future iterations may refactor this naming convention to reflect that they are more generally indices (e.g., from surveys). This may avoid confusion terminology that relates to fishery-dependent indices–i.e., CPUEs.\n\n\n\nThis method standardizes the CPUE values by subtracting the mean and dividing by the standard deviation across simulations:\n\\[\n\\text{score}_{i,t} = \\frac{\\text{CPUE}_{i,t} - \\mu_t}{\\sigma_t}\n\\]\nWhere:\n\\(\\mu_t\\) = mean CPUE in year \\(t\\) across simulations\n\\(\\sigma_t\\) = standard deviation in year \\(t\\) across simulations\nThis produces a unitless, centered score:\n\n\nShow the code\nscore &lt;- (met[, ac(dy[i])] %-% yearMeans(ref)) %/% sqrt(yearVars(ref))\n\n\nUseful when you want to assess relative anomalies in CPUE from expected trends.\n\n\n\nThis method compares the mean CPUE in recent years (dy) to a reference mean CPUE: \\[\n\\text{score}_i = \\frac{\\bar{\\text{CPUE}}_{dy, i}}{\\bar{\\text{CPUE}}_{ref, i}}\n\\] This is a relative index level and is not standardized:\n\n\nShow the code\nscore &lt;- yearMeans(met[, ac(dy[i])]) %/% yearMeans(ref)\n\n\nOften used when absolute differences in mean CPUE should affect TAC decisions.\n\n\n\nThis method passes through the CPUE values with no transformation:\n\\[\n\\text{score}_{i,t} = \\text{CPUE}_{i,t}\n\\]\n\n\nShow the code\nscore &lt;- met[, ac(dy[i])]\n\n\nUsed when raw or smoothed CPUE indices are deemed directly interpretable and comparable.\n\n\n\nThe following example shows how the three cpuescore methods behave using simulated CPUE data across 100 simulations and 10 years.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Simulate CPUE data\nset.seed(42)\nn_sim &lt;- 100\nn_year &lt;- 10\nind_sim &lt;- matrix(rlnorm(n_sim * n_year, meanlog = log(1), sdlog = 0.2), nrow = n_sim)\n\n# Reference: all years; \"recent\" period: years 9-10\nref_mean &lt;- rowMeans(ind_sim)\nref_sd &lt;- apply(ind_sim, 1, sd)\n\nz_scores &lt;- (ind_sim[, 10] - ref_mean) / ref_sd\nmean_scores &lt;- rowMeans(ind_sim[, 9:10]) / ref_mean\nraw_scores &lt;- ind_sim[, 10]\n\n# Reshape for plotting\nscore_df &lt;- tibble(\n  sim = 1:n_sim,\n  z = z_scores,\n  mean = mean_scores,\n  level = raw_scores\n) %&gt;% pivot_longer(cols = -sim, names_to = \"score_type\", values_to = \"score\")\n\nggplot(score_df, aes(x = sim, y = score)) +\n  geom_line() +\n  # facet_wrap(~score_type, scales = \"free_y\") +\n  facet_wrap(~score_type) +\n  labs(\n    title = \"Simulated CPUE Score Types\",\n    x = \"Simulation\",\n    y = \"Score Value\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nThe indices were refactored from the cpuescore functions to avoid confusion with surveys and can be summarized as follows:\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nNormalized\nUse case\n\n\n\n\nindscore.z\nStandardize vs mean/sd\n✅\nCompare anomalies across sims\n\n\nindscore.mean\nMean ratio of dy vs ref\n❌\nIndex levels matter\n\n\nindscore.level\nRaw CPUE values used as-is\n❌\nWhen index is well-calibrated\n\n\n\n⸻\n\n\nShow the code\nindscore.z &lt;- function(\n    stk, idx, index = 1, dlag = rep(args$data_lag, length(index)),\n    refyrs = NULL, args, tracking) {\n  dlag &lt;- setNames(dlag, nm = names(idx)[index])\n  ay &lt;- args$ay\n  dy &lt;- ay - dlag\n  res &lt;- as.list(setNames(nm = names(idx)[index]))\n\n  for (i in names(res)) {\n    met &lt;- window(idx[[i]], end = dy[i]) # removed biomass()\n\n    ref &lt;- if (!is.null(refyrs)) met[, ac(refyrs)] else met\n\n    res[[i]] &lt;- (met[, ac(dy[i])] %-% yearMeans(ref)) %/% sqrt(yearVars(ref))\n\n    dimnames(res[[i]])$year &lt;- max(dy)\n\n    track(tracking, paste0(\"score.mean.\", i), ac(ay)) &lt;- yearMeans(ref)\n    track(tracking, paste0(\"score.sd.\", i), ac(ay)) &lt;- sqrt(yearVars(ref))\n    track(tracking, paste0(\"score.ind.\", i), ac(ay)) &lt;- res[[i]]\n  }\n\n  return(list(stk = stk, ind = res, tracking = tracking, cpue = met))\n}\nbufferdelta.hcr &lt;- function(stk, ind, target = 0.5, width = 1,\n                            sloperatio = 0.2, dupp = NULL, dlow = NULL,\n                            args, tracking) {\n  # setup\n  ay &lt;- args$ay\n  iy &lt;- args$iy\n  frq &lt;- args$frq\n  man_lag &lt;- args$management_lag\n  cys &lt;- seq(ay + man_lag, ay + man_lag + frq - 1)\n\n  # compute score\n  score &lt;- ind[[1]] # assuming single FLQuant named 'zscore', 'mean_ratio', or 'level'\n\n  # define slope\n  bufflow &lt;- target - width\n  buffupp &lt;- target + width\n  lim &lt;- target - 2 * width\n  slope &lt;- sloperatio / width\n\n  # compute h(score)\n  h &lt;- ifelse(score &lt; lim, 0,\n    ifelse(score &lt; bufflow,\n      slope * (score - lim),\n      ifelse(score &gt; buffupp, 1 + sloperatio * (score - buffupp), 1)\n    )\n  )\n\n  tac_prev &lt;- catch(stk)[, ac(iy - 1)]\n  tac_new &lt;- tac_prev * h\n\n  # apply TAC limits if given\n  if (!is.null(dupp)) tac_new &lt;- pmin(tac_new, tac_prev * dupp)\n  if (!is.null(dlow)) tac_new &lt;- pmax(tac_new, tac_prev * dlow)\n\n  catch(stk)[, ac(cys)] &lt;- tac_new\n  return(list(stk = stk, ind = ind, tracking = tracking))\n}\n\nargs &lt;- list(ay = 2025, iy = 2026, frq = 1, data_lag = 1, management_lag = 1)\nidx &lt;- list(\"cpue\" = FLQuant(rlnorm(30), dimnames = list(year = 1996:2025)))\n\nstk &lt;- FLStock(catch = FLQuant(1000, dimnames = list(year = 2025:2030)))\n\nresult &lt;- indscore.z(stk, idx, args = args, tracking = FLQuant(0))\noutput &lt;- bufferdelta.hcr(result$stk, result$ind, args = args, tracking = result$tracking)\n\ncatch(output$stk)\n\nargs &lt;- list(ay = 2025, iy = 2026, frq = 1, data_lag = 1, management_lag = 1)\nidx &lt;- list(\"cpue\" = FLQuant(rlnorm(30), dimnames = list(year = 1996:2025)))\n\nresult &lt;- indscore.z(stk, idx, args = args, tracking = FLQuant(0))\noutput &lt;- bufferdelta.hcr(result$stk, result$ind, args = args, tracking = result$tracking)\n\n\n\n\n\n\n\n\n\n\n\n\nObject Name\nType / Purpose\n\n\n\n\nh1\nA list containing the full OM, OEM, and IEM for hypothesis H1 (qs file)\n\n\nom\nIterated subset of the Operating Model from h1\n\n\noem, iem\nObservation and implementation error models; extracted from h1\n\n\nomperf\nPerformance metrics of OM alone, usually C, F, SB for conditioning years\n\n\nperf\nCombined data frame of MP simulation performance results\n\n\ngetSlick\nFunction that merges MP/OM results and constructs a Slick summary object\n\n\nFLslick\nConstructor function that builds and returns a Slick object for plotting\n\n\nsli\nThe returned Slick object for visualization (Kobe, Quilt, Spider, etc.)\n\n\nctrl\nA list of control parameters for MPs (e.g., estimation methods, tuning devs)\n\n\ncondition\nNot found in current project files; possibly a misidentified object\n\n\n\n\n\nThe Slick software, in addition to providing an interface for examining OMs and MPs, also provides some helper functions to visualize the results of the MPs simulations directly. The examples in the following code chunk illustrate some of these features.\n\n\nShow the code\nsli &lt;- readRDS(here::here(\"demo\", \"PayaShortCuts_2.slick\"))\n\nplotQuilt(sli, kable=TRUE, signif=2)\nplotKobe(sli, Time=TRUE, xmax=1.8, ymax=1.2)\nplotSpider(sli )\n\nplotTradeoff(FilterSlick(sli, MPs=c(2,6,10), plot='Tradeoff'), c(1,1,1,1), c(2,12,4,5))\nplotTradeoff(FilterSlick(sli, MPs=1:4, plot='Tradeoff'), c(1,1,1,1), c(2,12,4,5))\nplotTimeseries(FilterSlick(sli, MPs=1:4, plot='Timeseries'), PI=2, includeHist=FALSE)\nplotTimeseries(FilterSlick(sli, MPs=1:4, plot='Timeseries'), PI=3, includeHist=FALSE)\nplotTimeseries(FilterSlick(sli, MPs=c(2,6,10), plot='Timeseries'), includeHist=FALSE, PI=2)\n\n\n\n\n\nThe Slick object is the core summary container created by the getSlick() and FLslick() functions. It contains performance data used for visualization and evaluation across multiple Management Procedures (MPs) and Operating Models (OMs).\n\n\n\nSlot\nContents\n\n\n\n\n@Boxplot\nMP × OM × performance indicators (boxplots)\n\n\n@Kobe\nSB/SBMSY vs F/FMSY over kobeyrs\n\n\n@Quilt\nHeatmap of average performance\n\n\n@Spider\nScaled performance for visual trade-offs\n\n\n@Timeseries\nTime series of F, C, SB\n\n\n@Tradeoff\nMean trade-off indicators (post-OM years)\n\n\n@MPs, @OMs\nMetadata: MP and OM definitions and labels\n\n\n\n\nThe R code below demonstrates how to load the operating model (OM) and compute baseline performance metrics so that the OM performance with MP results using the getSlick() function can be run.\n\n\nShow the code\n# Load OM and compute baseline performance\nh1 &lt;- om #qread(\"data/h1_1.07.qs\")\nom &lt;- iter(h1$om, seq(100))\nomperf &lt;- performance(om, years = 1970:2023, statistics = statistics[c(\"C\", \"F\", \"SB\")])\n\nperf &lt;- readPerformance(here::here(\"demo\",\"performance.dat.gz\"))\nhead(perf)\nhead(omperf)\nsummary(omperf)\n\n# Combine with MP results (perf), filtering to \"tune\" runs\nsli &lt;- getSlick(perf, omperf, kobeyrs = 2034:2042)\nsli &lt;- getSlick(perf[grep(\"tune\", run)], omperf, kobeyrs = 2034:2042)",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#defining-management-procedures-using-mpctrl-and-msectrl",
    "href": "doc/App_E.html#defining-management-procedures-using-mpctrl-and-msectrl",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "In the mse package, Management Procedures (MPs) are constructed as modular sequences of functional components that simulate how a fishery would be managed under alternative strategies. These strategies are defined using mpCtrl, which organizes component modules defined via mseCtrl.\nThis modular design allows you to:\n\nSelect estimation methods for stock status (est)\nDefine harvest control rules (hcr, phcr)\nSimulate implementation systems (isys)\nInclude optional technical measures (tm)\n\n\n\nThe mpCtrl() constructor takes a named list of components. Each component must be an mseCtrl object that defines the function to use (method) and its input parameters (args).\n\n\nShow the code\nctrl &lt;- mpCtrl(list(\n  est = mseCtrl(method = shortcut.sa, args = list(\n    metric = \"depletion\",\n    devs = metdevs,\n    B0 = refpts(om)$SB0\n  )),\n  hcr = mseCtrl(method = buffer.hcr, args = list(\n    target  = 1000,\n    bufflow = 0.30,\n    buffupp = 0.50,\n    lim     = 0.10,\n    min     = 0,\n    metric  = \"depletion\"\n  )),\n  isys = mseCtrl(method = split.is, args = list(\n    split = catch_props(om)$last5\n  ))\n))\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nest\nThe estimator module determines how the stock status is assessed during each management cycle.\n\n\nhcr\nThe harvest control rule uses the estimated status to recommend a total allowable catch or effort level.\n\n\nphcr\n(Optional) Parametrization helper for complex or adaptive control rules.\n\n\nisys\nSimulates how the recommended catch is implemented across fleets, often using historical proportions or allocation logic.\n\n\ntm\n(Optional) Includes non-catch-based rules, such as minimum size limits or gear restrictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\nExample method\nCommon args\n\n\n\n\nest\nperfect.sa, shortcut.sa\nmetric, devs, B0, years\n\n\nhcr\nbuffer.hcr, hockeystick.hcr, trend.hcr\ntarget, lim, bufflow, buffupp, trigger, metric\n\n\nisys\nsplit.is, fixed.is\nsplit, noise, bias\n\n\ntm\nuser-defined\nsize or spatial constraints\n\n\n\n\n\n\nEach mseCtrl specifies a method function and its arguments.\n\n\nShow the code\nctl &lt;- mseCtrl(\n  method = buffer.hcr,\n  args = list(\n    target  = 1000,\n    bufflow = 0.30,\n    buffupp = 0.50,\n    lim     = 0.10,\n    min     = 0,\n    metric  = \"depletion\"\n  )\n)\n\n\n\n\n\nYou can programmatically access or modify components of an mpCtrl object:\n\n\nShow the code\n# Access or change the HCR method\nmethod(ctrl@hcr)\n\n# Update the target biomass value\nargs(ctrl@hcr)$target &lt;- 1200\n\n\nAlternatively, use accessor functions:\n\n\nShow the code\nhcr(ctrl) &lt;- mseCtrl(method = new.hcr, args = list(...))\nargs(ctrl, \"hcr\")$target &lt;- 1100\n\n\n\n\n\n\nThis framework enables:\n\nRapid prototyping of management strategies\nTransparent comparisons across MPs\nFlexible integration with simulated operating models\n\nBy separating each component of an MP into a function-object pair, the mse package supports reproducible, configurable, and extensible MSE design workflows.\nTo analyze the behavior of bufferdelta.hcr() over the range of index values used as input (i.e., the stock status metric like “depletion” or “zscore”), the key output to examine is the harvest control multiplier hcrm—which determines how much the TAC is adjusted relative to the previous TAC. This multiplier is a piecewise function of the index value at the data year.\n\n\n\nThis HCR formulation uses a smoothed transition based on a buffer zone around a biomass or metric target. The response scalar \\(h(m)\\), applied to the previous catch, is defined based on the relative metric value \\(m\\) (e.g., standardized index or depletion level), and follows a piecewise logic:\nLet:\n\n\\(m\\): observed metric (e.g., index value)\n\\(t\\): target level\n\\(w\\): buffer width\n\\(l = t - 2w\\): limit threshold\n\\(b_{\\text{low}} = t - w\\): buffer lower bound\n\\(b_{\\text{upp}} = t + w\\): buffer upper bound\n\\(r\\): slope ratio\n\nThen the Harvest Control Rule (HCR) response multiplier \\(h(m)\\) is:\n\\[\nh(m) =\n\\begin{cases}\n\\frac{1}{2} \\left(\\frac{m}{l}\\right)^2, & \\text{if } m \\leq l \\\\\n\\frac{1}{2} \\left(1 + \\frac{m - l}{b_{\\text{low}} - l} \\right), & \\text{if } l &lt; m &lt; b_{\\text{low}} \\\\\n1, & \\text{if } b_{\\text{low}} \\leq m &lt; b_{\\text{upp}} \\\\\n1 + r \\cdot \\frac{1}{2(b_{\\text{low}} - l)} (m - b_{\\text{upp}}), & \\text{if } m \\geq b_{\\text{upp}} \\\\\n\\end{cases}\n\\]\nThe resulting Total Allowable Catch (TAC) is calculated as:\n\\[\n\\text{TAC}_{\\text{new}} = \\text{TAC}_{\\text{previous}} \\cdot h(m)\n\\]\nOptional constraints on TAC changes can be applied:\n\\[\n\\text{TAC}_{\\text{new}} = \\min(\\text{TAC}_{\\text{new}}, \\text{TAC}_{\\text{previous}} \\cdot d_{\\text{upp}})\n\\]\n\\[\n\\text{TAC}_{\\text{new}} = \\max(\\text{TAC}_{\\text{new}}, \\text{TAC}_{\\text{previous}} \\cdot d_{\\text{low}})\n\\]\n\n\n\nLet’s denote:\n\\(m\\): index metric (e.g., depletion) at the data year\n\\(\\text{target}\\): central value, e.g., 0.5\n\\(\\text{width}\\): buffer width, e.g., 1.0\nThen:\n\\(\\text{bufflow} = \\text{target} - \\text{width}\\)\n\\(\\text{buffupp} = \\text{target} + \\text{width}\\)\n\\(\\text{lim} = \\text{target} - 2 \\cdot \\text{width}\\)\n\n\n\n\n\n\n\n\n\n\n\nIndex value \\(m\\)\nDescription\nMultiplier h(m)\nTAC behavior\n\n\n\n\n\\(m \\leq \\text{lim}\\)\nVery low index\nQuadratic increase from 0 → 0.5\nStrong reduction\n\n\n\\(\\text{lim} &lt; m &lt; \\text{bufflow}\\)\nBetween limit and lower buffer\nLinear rise from 0.5 to 1\nModerate reduction\n\n\n\\(\\text{bufflow} \\leq m &lt; \\text{buffupp}\\)\nWithin buffer zone\nFlat at 1\nNo change\n\n\n\\(m \\geq \\text{buffupp}\\)\nAbove upper buffer\nLinear increase starting at 1 (slope = sloperatio)\nModerate increase\n\n\n\n\n\n\nIf you use the defaults:\ntarget = 0.5\nwidth = 1\nsloperatio = 0.2\nThen:\n•   bufflow = -0.5, buffupp = 1.5, lim = -1.5\n\n•   The flat zone is from -0.5 to 1.5 (note: with depletion metric, this wide range makes sense for standardized metrics like z-scores, but not raw depletion)\nIf using depletion as the metric, you’d typically want:\ntarget = 0.4\nwidth = 0.1\nsloperatio = 0.2\n→ lim = 0.2, bufflow = 0.3, buffupp = 0.5 So the response curve looks like:\n\n\nShow the code\nplot_hcrm &lt;- function(target = 0.4, width = 0.1, sloperatio = 0.2, metric_range = seq(0, 1, 0.01)) {\n  bufflow &lt;- target - width\n  buffupp &lt;- target + width\n  lim &lt;- target - 2 * width\n\n  hcrm &lt;- ifelse(metric_range &lt;= lim,\n    ((metric_range / lim)^2) / 2,\n    ifelse(metric_range &lt; bufflow,\n      0.5 * (1 + (metric_range - lim) / (bufflow - lim)),\n      ifelse(metric_range &lt; buffupp,\n        1,\n        1 + sloperatio * 1 / (2 * (bufflow - lim)) * (metric_range - buffupp)\n      )\n    )\n  )\n\n  plot(metric_range, hcrm,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Metric (e.g., depletion)\", ylab = \"Harvest multiplier (hcrm)\",\n    main = \"Response of TAC to Index Metric\"\n  )\n  abline(h = 1, col = \"gray\", lty = 2)\n  abline(v = c(lim, bufflow, buffupp), col = \"red\", lty = 3)\n}\nplot_hcrm(target = 0.4, width = 0.1, sloperatio = 0.2)\n\n\n\n\n\n\n\n\n\nThis shows the piecewise nature of the multiplier and can be tailored to any input metric (depletion, zscore, etc.).",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#overview-of-cpuescore",
    "href": "doc/App_E.html#overview-of-cpuescore",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "In the jmMSE framework, different CPUE scoring functions are used to inform harvest control rules (HCRs). These functions standardize or compare CPUE time series across simulations and reference periods. The three primary scoring methods are:\n\ncpuescore.z\ncpuescore.mean\ncpuescore.level\n\nThese names were modified from the original cpuescore functions to provide more clarity. Future iterations may refactor this naming convention to reflect that they are more generally indices (e.g., from surveys). This may avoid confusion terminology that relates to fishery-dependent indices–i.e., CPUEs.",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#z-score-standardization-cpuescore.z",
    "href": "doc/App_E.html#z-score-standardization-cpuescore.z",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "This method standardizes the CPUE values by subtracting the mean and dividing by the standard deviation across simulations:\n\\[\n\\text{score}_{i,t} = \\frac{\\text{CPUE}_{i,t} - \\mu_t}{\\sigma_t}\n\\]\nWhere:\n\\(\\mu_t\\) = mean CPUE in year \\(t\\) across simulations\n\\(\\sigma_t\\) = standard deviation in year \\(t\\) across simulations\nThis produces a unitless, centered score:\n\n\nShow the code\nscore &lt;- (met[, ac(dy[i])] %-% yearMeans(ref)) %/% sqrt(yearVars(ref))\n\n\nUseful when you want to assess relative anomalies in CPUE from expected trends.",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#mean-ratio-cpuescore.mean",
    "href": "doc/App_E.html#mean-ratio-cpuescore.mean",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "This method compares the mean CPUE in recent years (dy) to a reference mean CPUE: \\[\n\\text{score}_i = \\frac{\\bar{\\text{CPUE}}_{dy, i}}{\\bar{\\text{CPUE}}_{ref, i}}\n\\] This is a relative index level and is not standardized:\n\n\nShow the code\nscore &lt;- yearMeans(met[, ac(dy[i])]) %/% yearMeans(ref)\n\n\nOften used when absolute differences in mean CPUE should affect TAC decisions.",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#raw-index-cpuescore.level",
    "href": "doc/App_E.html#raw-index-cpuescore.level",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "This method passes through the CPUE values with no transformation:\n\\[\n\\text{score}_{i,t} = \\text{CPUE}_{i,t}\n\\]\n\n\nShow the code\nscore &lt;- met[, ac(dy[i])]\n\n\nUsed when raw or smoothed CPUE indices are deemed directly interpretable and comparable.",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#example-simulated-scores",
    "href": "doc/App_E.html#example-simulated-scores",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "The following example shows how the three cpuescore methods behave using simulated CPUE data across 100 simulations and 10 years.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Simulate CPUE data\nset.seed(42)\nn_sim &lt;- 100\nn_year &lt;- 10\nind_sim &lt;- matrix(rlnorm(n_sim * n_year, meanlog = log(1), sdlog = 0.2), nrow = n_sim)\n\n# Reference: all years; \"recent\" period: years 9-10\nref_mean &lt;- rowMeans(ind_sim)\nref_sd &lt;- apply(ind_sim, 1, sd)\n\nz_scores &lt;- (ind_sim[, 10] - ref_mean) / ref_sd\nmean_scores &lt;- rowMeans(ind_sim[, 9:10]) / ref_mean\nraw_scores &lt;- ind_sim[, 10]\n\n# Reshape for plotting\nscore_df &lt;- tibble(\n  sim = 1:n_sim,\n  z = z_scores,\n  mean = mean_scores,\n  level = raw_scores\n) %&gt;% pivot_longer(cols = -sim, names_to = \"score_type\", values_to = \"score\")\n\nggplot(score_df, aes(x = sim, y = score)) +\n  geom_line() +\n  # facet_wrap(~score_type, scales = \"free_y\") +\n  facet_wrap(~score_type) +\n  labs(\n    title = \"Simulated CPUE Score Types\",\n    x = \"Simulation\",\n    y = \"Score Value\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#summary-1",
    "href": "doc/App_E.html#summary-1",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "The indices were refactored from the cpuescore functions to avoid confusion with surveys and can be summarized as follows:\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nNormalized\nUse case\n\n\n\n\nindscore.z\nStandardize vs mean/sd\n✅\nCompare anomalies across sims\n\n\nindscore.mean\nMean ratio of dy vs ref\n❌\nIndex levels matter\n\n\nindscore.level\nRaw CPUE values used as-is\n❌\nWhen index is well-calibrated\n\n\n\n⸻\n\n\nShow the code\nindscore.z &lt;- function(\n    stk, idx, index = 1, dlag = rep(args$data_lag, length(index)),\n    refyrs = NULL, args, tracking) {\n  dlag &lt;- setNames(dlag, nm = names(idx)[index])\n  ay &lt;- args$ay\n  dy &lt;- ay - dlag\n  res &lt;- as.list(setNames(nm = names(idx)[index]))\n\n  for (i in names(res)) {\n    met &lt;- window(idx[[i]], end = dy[i]) # removed biomass()\n\n    ref &lt;- if (!is.null(refyrs)) met[, ac(refyrs)] else met\n\n    res[[i]] &lt;- (met[, ac(dy[i])] %-% yearMeans(ref)) %/% sqrt(yearVars(ref))\n\n    dimnames(res[[i]])$year &lt;- max(dy)\n\n    track(tracking, paste0(\"score.mean.\", i), ac(ay)) &lt;- yearMeans(ref)\n    track(tracking, paste0(\"score.sd.\", i), ac(ay)) &lt;- sqrt(yearVars(ref))\n    track(tracking, paste0(\"score.ind.\", i), ac(ay)) &lt;- res[[i]]\n  }\n\n  return(list(stk = stk, ind = res, tracking = tracking, cpue = met))\n}\nbufferdelta.hcr &lt;- function(stk, ind, target = 0.5, width = 1,\n                            sloperatio = 0.2, dupp = NULL, dlow = NULL,\n                            args, tracking) {\n  # setup\n  ay &lt;- args$ay\n  iy &lt;- args$iy\n  frq &lt;- args$frq\n  man_lag &lt;- args$management_lag\n  cys &lt;- seq(ay + man_lag, ay + man_lag + frq - 1)\n\n  # compute score\n  score &lt;- ind[[1]] # assuming single FLQuant named 'zscore', 'mean_ratio', or 'level'\n\n  # define slope\n  bufflow &lt;- target - width\n  buffupp &lt;- target + width\n  lim &lt;- target - 2 * width\n  slope &lt;- sloperatio / width\n\n  # compute h(score)\n  h &lt;- ifelse(score &lt; lim, 0,\n    ifelse(score &lt; bufflow,\n      slope * (score - lim),\n      ifelse(score &gt; buffupp, 1 + sloperatio * (score - buffupp), 1)\n    )\n  )\n\n  tac_prev &lt;- catch(stk)[, ac(iy - 1)]\n  tac_new &lt;- tac_prev * h\n\n  # apply TAC limits if given\n  if (!is.null(dupp)) tac_new &lt;- pmin(tac_new, tac_prev * dupp)\n  if (!is.null(dlow)) tac_new &lt;- pmax(tac_new, tac_prev * dlow)\n\n  catch(stk)[, ac(cys)] &lt;- tac_new\n  return(list(stk = stk, ind = ind, tracking = tracking))\n}\n\nargs &lt;- list(ay = 2025, iy = 2026, frq = 1, data_lag = 1, management_lag = 1)\nidx &lt;- list(\"cpue\" = FLQuant(rlnorm(30), dimnames = list(year = 1996:2025)))\n\nstk &lt;- FLStock(catch = FLQuant(1000, dimnames = list(year = 2025:2030)))\n\nresult &lt;- indscore.z(stk, idx, args = args, tracking = FLQuant(0))\noutput &lt;- bufferdelta.hcr(result$stk, result$ind, args = args, tracking = result$tracking)\n\ncatch(output$stk)\n\nargs &lt;- list(ay = 2025, iy = 2026, frq = 1, data_lag = 1, management_lag = 1)\nidx &lt;- list(\"cpue\" = FLQuant(rlnorm(30), dimnames = list(year = 1996:2025)))\n\nresult &lt;- indscore.z(stk, idx, args = args, tracking = FLQuant(0))\noutput &lt;- bufferdelta.hcr(result$stk, result$ind, args = args, tracking = result$tracking)",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_E.html#environment-objects",
    "href": "doc/App_E.html#environment-objects",
    "title": "Appendix E. Notes on the Jack Mackerel MSE Framework",
    "section": "",
    "text": "Object Name\nType / Purpose\n\n\n\n\nh1\nA list containing the full OM, OEM, and IEM for hypothesis H1 (qs file)\n\n\nom\nIterated subset of the Operating Model from h1\n\n\noem, iem\nObservation and implementation error models; extracted from h1\n\n\nomperf\nPerformance metrics of OM alone, usually C, F, SB for conditioning years\n\n\nperf\nCombined data frame of MP simulation performance results\n\n\ngetSlick\nFunction that merges MP/OM results and constructs a Slick summary object\n\n\nFLslick\nConstructor function that builds and returns a Slick object for plotting\n\n\nsli\nThe returned Slick object for visualization (Kobe, Quilt, Spider, etc.)\n\n\nctrl\nA list of control parameters for MPs (e.g., estimation methods, tuning devs)\n\n\ncondition\nNot found in current project files; possibly a misidentified object\n\n\n\n\n\nThe Slick software, in addition to providing an interface for examining OMs and MPs, also provides some helper functions to visualize the results of the MPs simulations directly. The examples in the following code chunk illustrate some of these features.\n\n\nShow the code\nsli &lt;- readRDS(here::here(\"demo\", \"PayaShortCuts_2.slick\"))\n\nplotQuilt(sli, kable=TRUE, signif=2)\nplotKobe(sli, Time=TRUE, xmax=1.8, ymax=1.2)\nplotSpider(sli )\n\nplotTradeoff(FilterSlick(sli, MPs=c(2,6,10), plot='Tradeoff'), c(1,1,1,1), c(2,12,4,5))\nplotTradeoff(FilterSlick(sli, MPs=1:4, plot='Tradeoff'), c(1,1,1,1), c(2,12,4,5))\nplotTimeseries(FilterSlick(sli, MPs=1:4, plot='Timeseries'), PI=2, includeHist=FALSE)\nplotTimeseries(FilterSlick(sli, MPs=1:4, plot='Timeseries'), PI=3, includeHist=FALSE)\nplotTimeseries(FilterSlick(sli, MPs=c(2,6,10), plot='Timeseries'), includeHist=FALSE, PI=2)\n\n\n\n\n\nThe Slick object is the core summary container created by the getSlick() and FLslick() functions. It contains performance data used for visualization and evaluation across multiple Management Procedures (MPs) and Operating Models (OMs).\n\n\n\nSlot\nContents\n\n\n\n\n@Boxplot\nMP × OM × performance indicators (boxplots)\n\n\n@Kobe\nSB/SBMSY vs F/FMSY over kobeyrs\n\n\n@Quilt\nHeatmap of average performance\n\n\n@Spider\nScaled performance for visual trade-offs\n\n\n@Timeseries\nTime series of F, C, SB\n\n\n@Tradeoff\nMean trade-off indicators (post-OM years)\n\n\n@MPs, @OMs\nMetadata: MP and OM definitions and labels\n\n\n\n\nThe R code below demonstrates how to load the operating model (OM) and compute baseline performance metrics so that the OM performance with MP results using the getSlick() function can be run.\n\n\nShow the code\n# Load OM and compute baseline performance\nh1 &lt;- om #qread(\"data/h1_1.07.qs\")\nom &lt;- iter(h1$om, seq(100))\nomperf &lt;- performance(om, years = 1970:2023, statistics = statistics[c(\"C\", \"F\", \"SB\")])\n\nperf &lt;- readPerformance(here::here(\"demo\",\"performance.dat.gz\"))\nhead(perf)\nhead(omperf)\nsummary(omperf)\n\n# Combine with MP results (perf), filtering to \"tune\" runs\nsli &lt;- getSlick(perf, omperf, kobeyrs = 2034:2042)\nsli &lt;- getSlick(perf[grep(\"tune\", run)], omperf, kobeyrs = 2034:2042)",
    "crumbs": [
      "Appendices",
      "Misc. notes"
    ]
  },
  {
    "objectID": "doc/App_C.html",
    "href": "doc/App_C.html",
    "title": "Appendix C, minutes",
    "section": "",
    "text": "The workshop included both in-person and online participation, with representatives from Peru, Chile, Argentina, Ecuador, and the Netherlands, among others.\nThe agenda was described as ambitious, with a focus on hands-on technical work, including software installation and repository access.\n\n\n\n\n\nTwo main GitHub repositories are central: FLjjm (for building FLR objects and running JJM inside the MP) and jjmMSE (the main development site for the MSE work).\nThe workflow follows the DAF (Data, Analysis, Framework) system, with clear steps for data preparation, OEM (Operating Model) conditioning, and performance analysis.\nEmphasis was placed on forking repositories and using branches for collaborative work, with a preference for merging at the end of the week.\nDocker was suggested as a potential solution for ensuring consistent environments across operating systems and participants, though not yet implemented.\n\n\n\n\n\nThe group is working with both single-stock and two-stock hypotheses, including a two-stock model with future-applied connectivity based on movement matrices.\nRobustness scenarios are being explored, including cyclic environmental changes and their impacts on productivity.\nThe group discussed the use of “.q” files for efficient storage and handling of large simulation outputs.\n\n\n\n\n\nThe calculation and interpretation of MSY (Maximum Sustainable Yield) and FMSY were debated, with concerns about the realism of current methods in JJM.\nIt was agreed that the 10-year average of MSY reference points would be used for performance evaluation, to avoid short-term volatility.\nSelectivity patterns and their impact on projections were a major topic. The group considered transitioning from terminal year selectivity to long-term averages over a five-year period to avoid unrealistic jumps in catch projections.\nThere was consensus that the main focus should be on long-term performance of management procedures, but short- and mid-term results are also important for managers.\n\n\n\n\n\nThe workshop addressed the need to model environmental variability, particularly El Niño events, and their impact on stock productivity, weight-at-age, and selectivity.\nLiterature-informed scenarios were presented, with parameters for changes in mortality, recruitment, and spatial distribution.\nRegional differences (e.g., between far north and south stocks) and their implications for catchability and biological responses were discussed.\n\n\n\n\n\nParticipants will review and potentially refine the environmental scenarios, with a focus on realism and literature support.\nFurther work is planned on selectivity transitions and the technical implementation of gradual changes.\nThe group will continue to test and validate the workflow, with an emphasis on reproducibility and collaborative code development.",
    "crumbs": [
      "Appendices",
      "Minutes"
    ]
  },
  {
    "objectID": "doc/App_C.html#day-1-summary",
    "href": "doc/App_C.html#day-1-summary",
    "title": "Appendix C, minutes",
    "section": "",
    "text": "The workshop included both in-person and online participation, with representatives from Peru, Chile, Argentina, Ecuador, and the Netherlands, among others.\nThe agenda was described as ambitious, with a focus on hands-on technical work, including software installation and repository access.\n\n\n\n\n\nTwo main GitHub repositories are central: FLjjm (for building FLR objects and running JJM inside the MP) and jjmMSE (the main development site for the MSE work).\nThe workflow follows the DAF (Data, Analysis, Framework) system, with clear steps for data preparation, OEM (Operating Model) conditioning, and performance analysis.\nEmphasis was placed on forking repositories and using branches for collaborative work, with a preference for merging at the end of the week.\nDocker was suggested as a potential solution for ensuring consistent environments across operating systems and participants, though not yet implemented.\n\n\n\n\n\nThe group is working with both single-stock and two-stock hypotheses, including a two-stock model with future-applied connectivity based on movement matrices.\nRobustness scenarios are being explored, including cyclic environmental changes and their impacts on productivity.\nThe group discussed the use of “.q” files for efficient storage and handling of large simulation outputs.\n\n\n\n\n\nThe calculation and interpretation of MSY (Maximum Sustainable Yield) and FMSY were debated, with concerns about the realism of current methods in JJM.\nIt was agreed that the 10-year average of MSY reference points would be used for performance evaluation, to avoid short-term volatility.\nSelectivity patterns and their impact on projections were a major topic. The group considered transitioning from terminal year selectivity to long-term averages over a five-year period to avoid unrealistic jumps in catch projections.\nThere was consensus that the main focus should be on long-term performance of management procedures, but short- and mid-term results are also important for managers.\n\n\n\n\n\nThe workshop addressed the need to model environmental variability, particularly El Niño events, and their impact on stock productivity, weight-at-age, and selectivity.\nLiterature-informed scenarios were presented, with parameters for changes in mortality, recruitment, and spatial distribution.\nRegional differences (e.g., between far north and south stocks) and their implications for catchability and biological responses were discussed.\n\n\n\n\n\nParticipants will review and potentially refine the environmental scenarios, with a focus on realism and literature support.\nFurther work is planned on selectivity transitions and the technical implementation of gradual changes.\nThe group will continue to test and validate the workflow, with an emphasis on reproducibility and collaborative code development.",
    "crumbs": [
      "Appendices",
      "Minutes"
    ]
  },
  {
    "objectID": "doc/App_C.html#day-2",
    "href": "doc/App_C.html#day-2",
    "title": "Appendix C, minutes",
    "section": "Day 2",
    "text": "Day 2\n\n0.7 Opening\n\nThe workshop began with participant introductions, including new attendees such as Robert Robinson.\nJim Ianelli welcomed participants, provided a recap of the previous day, and referenced a summary posted in the Teams channel for review.\n\n\n\n0.8 Review of Previous Work and Agenda\n\nThe agenda was described as ambitious, with a focus on technical aspects of Management Strategy Evaluation (MSE).\nDiscussion included the effects of El Niño on recruitment and catch distribution, referencing an unsent summary email and a report on the topic.\n\n\n\n0.9 Technical Discussions\n\na. Effects of El Niño and Distribution Shifts\n\nJim presented an analysis of catch distribution changes between coastal and offshore fleets, proposing a 15% average offshore drop (60% decline in Q for offshore fleet) and an 11% increase for coastal zones.\nParticipants debated the appropriateness of using long-term versus short-term averages and the need for smoothing changes rather than step changes.\nIt was agreed that the effect should be applied to both availability and catchability in simulations, with implications for the acoustic survey indices.\n\n\n\nb. Selectivity and Projection Periods\n\nThe group discussed the transition from recent selectivity estimates to long-term means, with concerns about artifacts from using 10-year averages.\nConsensus moved toward using a representative period (2000–2010) for selectivity, avoiding recent years with anomalously high selection for older fish.\nThe need for a smooth transition in selectivity assumptions for projections was emphasized.\n\n\n\nc. Recruitment Regimes and Projections\n\nAnalysis of recruitment means for projections considered the El Niño effect, with a proposed 23–30% bump up in recruitment for recent years.\nDebate ensued on which years to include for calculating means, with a focus on data consistency from 1991 onward.\nThe group discussed the potential for regime shifts and the implications for robustness testing in MSE.\n\n\n\nd. Model Implementation and Coding Practices\n\nDemonstrations were given on the use of R and package structures for running MSE simulations, including best practices for project setup and function sourcing.\nThe importance of consistent selectivity and Q parameter normalization across projections and indices was highlighted.\n\n\n\n\n0.10 Decisions and Action Items\n\nAdopt 2000–2010 as the reference period for selectivity in projections, with a smooth transition from current conditions.\nApply a 23–30% recruitment increase for projections reflecting recent El Niño effects, with final years to be confirmed.\nEnsure normalization of selectivity and Q parameters is consistent across all indices and projections.\nContinue refining the codebase, documenting changes, and sharing updates among the technical team.\n\n\n\n0.11 Other Business and Closing\n\nParticipants shared experiences with data handling, model setup, and coding challenges.\nThe workshop included informal discussions and technical clarifications.\nThe session concluded with plans to continue reviewing model outputs and performance indicators, and to reconvene as needed for further technical work.",
    "crumbs": [
      "Appendices",
      "Minutes"
    ]
  },
  {
    "objectID": "doc/App_C.html#day-3",
    "href": "doc/App_C.html#day-3",
    "title": "Appendix C, minutes",
    "section": "Day 3",
    "text": "Day 3\n\n0.12 Review of Technical Issues and Model Adjustments\n\nDiscussion focused on the technical aspects of Management Strategy Evaluation (MSE) for jack mackerel.\nParticipants examined the function and configuration of sliding buffers, control rules, and the impact of selectivity changes.\nThe group reviewed how indices (e.g., CPUE, acoustics) are incorporated, including the use of averages over multiple years and weighting schemes for recent data.\nThere was debate on the variance and correlation structure in observation models, and how these affect simulation results.\n\n\n\n0.13 Timing and Implementation of Management Procedures (MPs)\n\nThe workflow and timing for implementing MPs were clarified:\n\nData from 2026 would be used for advice in 2027.\nThe Scientific Committee (SC) would run the MP, with the Commission making final decisions.\nDiscussion on the need for preliminary versus finalized data and the implications for observation error.\n\nThe importance of using the most recent data versus the stability of multi-year averages was highlighted.\n\n\n\n0.14 Treatment of Effort Creep and Index Standardization\n\nParticipants agreed that simulating effort creep in future projections is not necessary for the base case, but could be a robustness test.\nThe importance of correcting historical indices for effort creep was emphasized, while future indices are assumed to be unbiased.\nThere was consensus that the standardized or corrected index should be used for both simulation and real-world application.\n\n\n\n0.15 Selection and Tuning of Management Procedures\n\nMultiple MPs were tested, each tuned to a 60% probability of meeting the Kobe green zone.\nThe group compared different buffer widths and TAC change limits, analyzing their effects on catch variability and performance metrics.\nDiscussion included the need for clear documentation of specifications and the importance of presenting a set of MPs with distinct trade-offs to the Commission, rather than recommending a single option.\n\n\n\n0.16 Performance Metrics and Projections\n\nThe team reviewed performance across near-term, medium-term, and long-term projections.\nBoxplots and other visualizations were used to compare MPs, focusing on catch variability, probability of stock being in the green zone, and interannual TAC changes.\nConcerns were raised about downward trends in some simulations and the need for additional performance statistics (e.g., probability of stock crash).\n\n\n\n0.17 Next Steps and Action Items\n\nAgreement to refine the OM projections and finalize Annex documentation on selectivity and other specifications.\nPlan to continue testing and tuning MPs, with further analysis of performance metrics.\nThe SC will present a set of MPs to the Commission, along with the status quo as a fallback.\nLunch arrangements and informal discussions concluded the session.",
    "crumbs": [
      "Appendices",
      "Minutes"
    ]
  },
  {
    "objectID": "doc/App_C.html#day-4",
    "href": "doc/App_C.html#day-4",
    "title": "Appendix C, minutes",
    "section": "Day 4",
    "text": "Day 4\n\n0.18 Key Activities and Discussions\n\nModel Runs and Debugging:\n\nOvernight and morning runs were conducted to examine the acoustics index using legacy targets and buffers.\nA significant focus was on debugging the generation of CPUE v3 for 2024 and 2025, investigating unexpected jumps in predicted values.\nAlternative normalization methods for CPUE were tested, following recommendations to improve stability.\n\nAnalysis of Index Jumps:\n\nThe team identified that the jump in the index from 2024 to 2025 was primarily driven by increases in vulnerable biomass and changes in mean weight at age, rather than selectivity changes alone.\nWeighted age calculations and their implications for projections were reviewed in detail, including the use of three-year means and the impact of preliminary data from 2024.\n\nCode Review and Live Demonstration:\n\nLive coding sessions were held to demonstrate how to adjust the OM to exclude problematic years (e.g., dropping 2024 and extending from 2023).\nSmoothing techniques for selectivity and weights at age were discussed and implemented to reduce artificial jumps in projections.\n\nUncertainty and Robustness:\n\nThe group discussed the treatment of process error, residual variability, and autocorrelation in indices.\nEmpirical approaches to setting CVs for indices were compared with default values, and the impact on future projections was considered.\n\nAction Items and Next Steps:\n\nFurther testing of the OM with adjusted years and smoothing is to be completed, with outputs to be pushed under new filenames to avoid disrupting ongoing work.\nContinued analysis of the causes of high catches in test runs and further parameter tuning were assigned.\nThe group agreed to revisit and possibly refine the approach to handling preliminary data and smoothing in both indices and weights at age.\n\n\n\n\n0.19 Notable Outcomes\n\nConsensus that both selectivity and mean weight at age contribute to observed index jumps, with smoothing and exclusion of problematic years being viable mitigation strategies.\nAgreement to document and communicate technical progress to the broader group, while maintaining a focus on robust, transparent modeling practices.",
    "crumbs": [
      "Appendices",
      "Minutes"
    ]
  },
  {
    "objectID": "doc/App_C.html#day-5",
    "href": "doc/App_C.html#day-5",
    "title": "Appendix C, minutes",
    "section": "Day 5",
    "text": "Day 5\n\n0.20 Model Runs and Technical Issues\n\nThe group reviewed progress on running various management procedures (MPs), focusing on the acoustic and CPUE indices. Issues with FL libraries and model reproducibility were discussed, with fixes applied to ensure models ran as expected.\nRobustness tests were conducted, particularly comparing the performance of different indices (acoustic, CPUE3, CPUE6, 3.6). The need to clarify how “shortcut” procedures treat stocks was debated, especially regarding biomass tracking and catch splits.\nThe group noted that tuning parameters (e.g., width, slope ratio) and their impact on model performance remain a challenge, especially when standardizing across indices.\n\n\n\n0.21 Interpretation and Presentation of Results\n\nThere was significant discussion about interpreting outputs, particularly when catch trends did not align with biomass trends. Concerns were raised about the credibility of certain indices and the need for clearer communication of model behavior.\nThe importance of visualizing trade-offs and the response of TAC to indices was emphasized, with ongoing efforts to develop summary figures for inclusion in reports.\n\n\n\n0.22 Workflows, Code, and Collaboration\n\nParticipants shared experiences with the codebase, noting progress in understanding and modifying functions, but also highlighting the need for further documentation and standardization.\nThe group agreed on the value of reproducibility and transparency, with suggestions to document daily progress and maintain clear records for future reference.\n\n\n\n0.23 Planning and Next Steps\n\nThe group recognized that while technical progress was made, the process is ongoing. There was consensus on the need for another technical workshop (ideally in person) to continue development and evaluation of MPs.\nThe timeline for delivering a report to the Scientific Committee (SC) and Commission was discussed, with acknowledgment that final recommendations are not yet possible. Instead, the report will focus on documenting progress, challenges, and a proposed work plan for the coming year.\nConcerns about funding, continuity (especially regarding software and contracts), and the need for member commitment to ongoing tool development were raised.\n\n\n\n0.24 Recommendations and Reflections\n\nThe group agreed to recommend continued development and evaluation of MPs, with an emphasis on transparency, reproducibility, and clear communication to managers.\nIt was noted that, if a new MP is not ready for 2026, the existing method (Annex K) will remain in use.\nThere was broad recognition of the complexity and time required for this process, and appreciation for the collaborative progress made during the workshop.",
    "crumbs": [
      "Appendices",
      "Minutes"
    ]
  },
  {
    "objectID": "doc/App_A.html",
    "href": "doc/App_A.html",
    "title": "Appendix A, participants",
    "section": "",
    "text": "Figure 1: Participants at the SCW15 technical workshop. From left to right; back row: Iago Mosqueira, Aquiles Sepulveda, Jose Zenteno, Josymar Torrejó; front row: Ricardo Oliveros-Ramos, Grant Adams, Ana Parma, Jim Ianelli, Ignacio Paya.",
    "crumbs": [
      "Appendices",
      "Participants"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SC13 - JM09 Report of the Management Strategy Evaluation (MSE) workshop (SCW15), July 14-18, 2025",
    "section": "",
    "text": "The Jack Mackerel Management Strategy Evaluation (MSE) Technical Workshop brought together scientists, technical experts, and external reviewers to assess recent progress and refine the MSE framework being developed under South Pacific Regional Fisheries Management Organisation (SPRFMO). The primary goal of the workshop was to ensure that the modeling framework  is scientifically sound, technically robust and transparent, and to produce a set of candidate management procedures to present to the 13th SC, as outlined by the Commission (e.g., SPRFMO Jack Mackerel Working Group (2025)). The workshop was successful in testing the framework, it resulted in improved understanding of scientists in its functioning and ability to test alternative MPs. Discussions during the workshop contributed to some changes made in the conditioning of the Operating Models (OMs) and in the algorithms of the harvest control rules to improve the robustness of the results. Further review and discussion are needed to agree on a set of candidate management procedures. Finally, the framework and preliminary results were accepted by the group as the basis for recommendations to the Science Committee.\n\n\n\nMSE Framework Consolidation\nParticipants reviewed the jmMSE software package, confirming that it provides a robust and flexible platform for conducting MSEs. The package includes 1) a reference set of operating models conditioned to historical data using Markov Chain Monte Carlo (MCMC; an efficient MP tuning algorithm), and 2) tools for visualizing and comparing results.\nMSE and OM Fine-Tuning\nThe workshop focused on refining the operating models (OMs) to ensure they accurately represent the jack mackerel stock dynamics and future behaviour. Participants discussed and implemented changes to the OM structure, including adjustments to recruitment dynamics and selectivity patterns. These refinements were aimed at improving the realism of the OMs and their ability to simulate plausible future scenarios.\nRobustness Testing\nThe workshop clarified the role and scope of robustness tests. These tests are intended to explore how candidate MPs perform under a range of plausible yet uncertain scenarios rather than represent definitive alternative models. Scenarios reflecting changes in recruitment, spatial availability, environmental regime shifts (e.g., El Niño Southern Oscillation (ENSO)), and stock structure were reviewed and refined for implementation.\nIndicator-driven MPs and harvest control rule (HCR) logic\nThe workshop evaluated empirical MPs based on one or more indicators, with focus on two formulations:\n\nTotal Allowable Catch (TAC) as a product of a target and a multiplier from an index (e.g. biomass from scientific surveys and /or commercial catch per unit effort (CPUE)).\nTAC adjusted incrementally from the previous year based on index signals.\n\nThe workshop tuned the MPs using the probability of being in the green zone of a Kobe plot (i.e., the biomass level is above the \\(B_{MSY}\\) and the fishing effort is below \\(F_{MSY}\\)) as an objective. All candidate MPs were tuned to achieve a probability value of 60%, a value identified using a questionaire distributed to members during COMM13.\n\n\n\nRecommendations and refinements\nThe participants recommended additional diagnostics and refinements, including:\n\nAdding plots of how index trajectories relate to TACs.\nNew performance metrics that reflect stock status and trends in the final projection years.\nEnsuring consistent treatment of selectivity, weights-at-age, and catch splits in both projections and reference point calculations.\nExploring robustness scenarios that account for variability in fleet selectivity and biological assumptions, particularly where CPUE is used as an input.\nExploring alternative HCRs.\n\nDocumentation and transparency\nThe participants emphasized the importance of transparency in documenting model assumptions, data sources, and MP structure. The group agreed on priorities for improving documentation and sharing annotated examples of MP behavior.\nNext steps and implementation\nThe next phase of this work will focus on finalizing the candidate MPs, running the robustness tests, and summarizing trade-offs across key performance indicators. In discussions, we also identified future reporting needs, including summary tables and figures for managers, and exploration of reference points and evaluation criteria beyond the green zone probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "index.html#simulating-el-niño-effects-in-the-operating-model",
    "href": "index.html#simulating-el-niño-effects-in-the-operating-model",
    "title": "SC13 - JM09 Report of the Management Strategy Evaluation (MSE) workshop (SCW15), July 14-18, 2025",
    "section": "4.1 Simulating El Niño effects in the Operating Model",
    "text": "4.1 Simulating El Niño effects in the Operating Model\nTo incorporate climate-driven variability into the Operating Model (OM) projections, we defined a scenario simulating El Niño–like events every five years beginning in 2030. These events affect primarily recruitment and distribution options. Based on the work of Iago and available literature, we proposed an initial set of biological and fishery processes that most likely relate to El Niño conditions. The group discussed these and noted that others may be best considered in the next round of stock assessment benchmark and for future MSE work.\nThe table below summarizes the proposed effects of simulated El Niño conditions on the OM, categorizing them by their expected direction, biological or fishery-based justification, and evaluation priority. The table is divided into two sections: effects that are prioritized for immediate evaluation and those deferred for further study.\nThe first section highlights two key El Niño-driven effects: 1) a 30% increase in recruitment with a one-year lag, linked to ENSO-related early life stage survival (Figure 2), and 2) shifts in catchability, with coastal regions experiencing increased availability and offshore regions seeing declines, reflecting observed onshore movement of fish during warm anomalies. The latter effect was identified as high-priority, with a focus on quantifying impacts on fishery removals.\nThe deferred effects included the potential for reduced weight-at-age (potentially due to prey scarcity), earlier maturity (a stress response observed in small pelagics), and increased natural mortality (from predation or environmental stress). These are flagged for future study, pending historical data checks or further evidence. The table succinctly organizes hypotheses while clarifying immediate next steps for the OM framework.\n\n\n\n\n\n\nFigure 2: Recruitment estimates and mean values (horizontal lines) used to estimate the impact of ENSO effects.\n\n\n\nThe following tables summarizes the key effects, their expected directions, and justification.\n\n\n\n\n\n\n\n\nEffect\nDirection\nJustification\n\n\n\n\nRecruitment ↑20%\n↑ 1-year lag\nENSO-linked early life stage effects on recruitment\n\n\nRegional availability\nCoast catchability ↑ and offshore ↓\nOnshore shift during warm anomalies\n\n\n\n\n\nDiscussed but deferred for further study\n\n\n\n\n\n\n\n\n\n\n\nEffect\nDirection\nJustification\nNotes\n\n\n\n\nWeight-at-age\n↓Productivity\nLower prey density and observed condition declines\nCheck historical WAA anomalies\n\n\nAge-1 maturity\nEarlier maturity\nStress response seen in small pelagics\nsimilar impact on Recruitment\n\n\nCoastal selectivity\nAge 1–2 sel ↑\nSpatial contraction, availability change\nConfirm from CPUE by age?\n\n\nM ↑30%/20%\n↓ Survival\nStress-induced mortality, predation\n\n\n\n\n\nEstimating relative availability from catch proportions\nTo estimate the relative availability of jack mackerel to different fleets, we analyzed catch proportion data from 2004 to 2024. These data were smoothed using a 5-year moving average to reduce annual variability. The data from the Chilean, Peruvian, and Ecuadorian fleets were classified as coastal, whereas the data from the other fleets were classified as “offshore” for the purpose of approximating changes in avaialbility.  The goal was to have some basis for defining changes in availability to coastal and offshore areas in a way that could roughly approximate the effective catchability \\(q\\) (which includes both true catchability and availability) of the fishery for each fleet, i.e.:\n\\[\n\\text{Catch}_{\\text{fleet}} \\propto q_{\\text{fleet}}\n\\]\nThus, observed catch proportions can serve as a proxy for relative availability of the resource to each fleet.\n\n5-year moving averages of the proportion of the catch occurring in the “coastal” areas compared to the offshore fleet.\n\n\nYear Range\nCoastal (%)\nOffshore (%)\n\n\n\n\n2004–2008\n81\n19\n\n\n2005–2009\n78\n22\n\n\n2006–2010\n74\n26\n\n\n2007–2011\n76\n24\n\n\n2008–2012\n78\n22\n\n\n2009–2013\n82\n18\n\n\n2010–2014\n84\n16\n\n\n2011–2015\n86\n14\n\n\n2012–2016\n86\n14\n\n\n2013–2017\n85\n15\n\n\n2014–2018\n86\n14\n\n\n2015–2019\n87\n13\n\n\n2016–2020\n91\n9\n\n\n2017–2021\n93\n7\n\n\n2018–2022\n94\n6\n\n\n2019–2023\n94\n6\n\n\n2020–2024\n94\n6\n\n\nMean\n85\n15\n\n\n\n\n\nRange of changes in estimated availability\nGiven the shift in catch proportions over this period, we can assume a relative catchability due to an environmental effect. We note that the coastal effective catchability increased from a low of 74% (2006–2010) to a high of 94% (2018–2024), representing 20 percentage point change. Meanwhile, the offshore effective catchability declined from a high of 26% (2006–2010) to a low of 6% (2018–2024), representing a decrease of 20%.\nAs part of the robustness test, we propose that the effective availability to the offshore fleet gradually drops from 15% of the mean biomass to 6% during El Niño periods (a 60% decline in \\(q\\) ). This would apply to the data generated for offshore CPUE index in the simulations. For the coastal zones, the effect of El Niño would correspond to an 11% increase in the availability of fish relative to the mean (85%). These changes would apply to the Chilean south-central CPUE index and the Peruvian CPUE index data generation. This is one proposal among many that could be imagined. For example, a slightly more conservative range could be based on the 10th and 90th percentiles of estimated effective catchability (from proportional catches):\nCoastal:\n•   10th percentile: 77%\n•   90th percentile: 94%\nOffshore:\n•   10th percentile: 6%\n•   90th percentile: 23%\nThese shifts may provide some scope for showing the impact of changes in the relative abundance indicators in index values. These reflect patterns over the past two decades, possibly due to environmental changes.",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "index.html#tuning-to-pgreen",
    "href": "index.html#tuning-to-pgreen",
    "title": "SC13 - JM09 Report of the Management Strategy Evaluation (MSE) workshop (SCW15), July 14-18, 2025",
    "section": "5.1 Tuning to P(Green)",
    "text": "5.1 Tuning to P(Green)\nIn Stage 1, a key activity is to “tune” MPs towards a common objective. Tuning MPs allows for more direct and straightforward comparisons of trade-offs between candidate MPs. The tuning objective was set as the probability of being in the green zone of the Kobe plot (i.e., the biomass level is above the \\(B_{MSY}\\) and the fishing effort is below \\(F_{MSY}\\)) for the final years of the projection period. This probability is denoted as (“P(Green)”). All candidate MPs were tuned to achieve a P(Green) value of 60%, a value identified using a questionaire distributed to members during COMM13.\nDuring the tuning process, the group noted that the current high stock status tended to increase catch levels, particularly at the beginning of the projection period. This can result in declining stock trends later in the projection period, even when the short-term performance criteria are met, since the current P(Green) is well above this 60% target. There were also difficulties ascertaining how the projected TACs were calculated from the empirical MPs that were being tested.\nWe also tuned MPs using short-cut assessment methods, mimicking the behaviour of the jjm model as used by the Jack Mackerel Working Group (JMWG) for advisory purposes. These short-cut approaches considered observations with relatively low uncertainty, high uncertainty and uncertainty with autocorrelation incorporated. Results using these short-cuts can be found in @Section 8.2.",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "index.html#further-recommendations-to-consider",
    "href": "index.html#further-recommendations-to-consider",
    "title": "SC13 - JM09 Report of the Management Strategy Evaluation (MSE) workshop (SCW15), July 14-18, 2025",
    "section": "6.1 Further recommendations to consider",
    "text": "6.1 Further recommendations to consider\n\nFor the SC:\n\nAdopt the current proposal structure (timeline and deliverables) with flexibility for future adjustment.\nContinue the review process to identify a shortlist of MP options to simplify the selection process at the Commission level.\nConsider a placeholder method for calculating the 2026 TAC since MSE work requires further development.\n\n\n\nFor Members:\n\nCommit to a shared MSE software base (FLR or openMSE).\nEngage in pre-SC online meetings to broaden participation in MSE discussions.\n\n\n\nFor Analyst (Iago):\n\nPrioritize enhancements discussed during the workshop:\n\nCode clarity and naming conventions\nLogical parameter usage across MPs\nRefinement of FLR-to-dataframe functions\n\nIdentify successor strategy after contract ends in 2025.",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "index.html#package-development",
    "href": "index.html#package-development",
    "title": "SC13 - JM09 Report of the Management Strategy Evaluation (MSE) workshop (SCW15), July 14-18, 2025",
    "section": "8.1 Package development",
    "text": "8.1 Package development\nThe SC Chair, Ricardo, developed some enhancements to the jmMSE demo framework after the workshop. These introduce greater flexibility and diagnostic power through two key components: performance2() and evaluate_mp().\nThe performance2() function extends standard summary outputs by computing a richer set of indicators. These indicators include mean relative biomass and fishing mortality, catch, the probability of remaining in the green zone of the Kobe plot, and the longest duration spent outside it. \nThe evaluate_mp() function wraps the full MP simulation while allowing multi-parameter optimization of Harvest Control Rule (HCR) settings. It also incorporates a customizable objective function that accounts for discounted catch, stability (via interannual catch variability (IACV)), conservation thresholds (e.g., probability of being “green”), and duration outside target reference points. This setup allows for filtering out of implausible simulations, facilitates adaptive MP tuning, and supports rapid exploration of trade-offs in management performance. Together, these tools make the MSE evaluation process more transparent, efficient, and tailored to decision-maker priorities.\n\nThis and other work conducted after the workshop encountered problems with the magnitude of recruitment variability which led to unreasonably high levels of biomass in a significant number of the simulations. This was something that required further investigation and resolution in collaboration with the developer (Iago).",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "index.html#sec-shortcut",
    "href": "index.html#sec-shortcut",
    "title": "SC13 - JM09 Report of the Management Strategy Evaluation (MSE) workshop (SCW15), July 14-18, 2025",
    "section": "8.2 Examples of Shortcut MPs",
    "text": "8.2 Examples of Shortcut MPs\n\nThis section was compiled by Ignacio Payá and colleagues from Chile. It summarizes an example implementation of “shortcut” management procedures (MPs) that simplify the estimation and harvest control rule (HCR) steps using buffered depletion-based control. It also illustrates how FLR-based tools can be used to define control logic, evaluate HCR targets, and visualize results using the Slick (Blue Matter Science (2024)) and mseviz (Mosqueira (2022)) R packages for performance evaluation. Each section below includes code, explanation, and figures that demonstrate specific aspects of the approach.\n\nShortcut MP Definition\nThis section defines the MP structure using mpCtrl() with shortcut estimation (shortcut.sa), a buffer-based HCR, and a split implementation system (ISYS). Deviations are defined using a lognormal AR(1) process.\n\n\nHCR Target Exploration\nTo evaluate MP performance under various Total Allowable Catch (TAC) values, the control object (ctrl) from Section 8.2.1 is tested across a set of TAC multipliers. The target range brackets the 2025 CMM level. In this example, the multipliers tested are 0.85, 1, 1.15, and 1.25 of the 2025 TAC. This produces comparative projection plots for four candidate TAC levels (Figure 4).\n\n\n\n\n\n\nFigure 4: Projections for TAC 2025 scenarios using buffer HCR.\n\n\n\nNow we test this with a simple limit on the annual change in TAC, which is a common requirement in many fisheries management systems. In other words, the TAC for a subsequent year cannot increase or decrease beyond a specified percentage. These results compare similarly to those without any TAC constraint (Figure 5). A clearer evaluation of these two sets is shown in the next section using the Slick package.\n\n\n\n\n\n\nFigure 5: Projections for TAC 2025 scenarios using buffer HCR with TAC change limit at 15 percent.\n\n\n\n\n\n\n\n\n\n\nProjections for TAC 2025 scenarios using buffer HCR with TAC change limit at 25% (downward) and 15% increases.\n\n\n\n\nShow code\nargs(ctrl$hcr)[c(\"dlow\", \"dupp\")] &lt;- c(0.85, 1.15)\ntargets_lim15 &lt;- mps(om, oem, ctrl=ctrl, args=mseargs,\n               hcr=list(target=c(TAC2025*0.85, TAC2025, TAC2025*1.15, TAC2025*1.25)))\npng(\"images/Projections_Sc_har_Around_TAC2025_lim.png\", width=800, height=450)\n  plot(om, targets_lim15) + ggthemes::theme_few() \ndev.off()\n##--Now with 25% change down, 15% uupper \nargs(ctrl$hcr)[c(\"dlow\", \"dupp\")] &lt;- c(0.75, 1.15)\ntargets_lim2515 &lt;- mps(om, oem, ctrl=ctrl, args=mseargs,\n               hcr=list(target=c(TAC2025*0.85, TAC2025, TAC2025*1.15, TAC2025*1.25)))\n\npng(\"images/Projections_Sc_har_Around_TAC2025_lim2515.png\", width=800, height=450)\n  plot(om, targets_lim2515) + ggthemes::theme_few() \ndev.off()\n##--Now with 25% change down, 15% uupper \n\n\n\n\nUsing openMSE’s Slick Output\nUsing the Slick package (Blue Matter Science (2024)), performance metrics such as catch, fishing mortality, and spawning biomass can be summarized across operating models and MPs. As one illustration of the application, we presents a Kobe-style status time series from Slick. We compare three shortcut management procedures (MPs) all tuned to the 2025 TAC level (1.55 million t) but differing in constraints on interannual TAC changes (Figure 6). The panels from left to right represent: (1) no constraint (1.0), (2) symmetric ±15% TAC change limit (1.0_d15), and (3) asymmetric −25%/+15% limit (1.0_d2515). Each panel shows the proportion of simulations over time falling into the green (safe), yellow (overfished or overfishing), and red (overfished and overfishing) zones. While all three MPs maintain a majority of simulations in the green zone, applying TAC constraints leads to a slight increase in the proportion of years falling into the red zone—particularly under the asymmetric constraint. This reflects the trade-off where increased catch stability may slightly elevate biological risk under certain scenarios. We provide a copy of the slick file on the repository under MS Teams at [tbd].\n\n\n\n\n\n\nFigure 6: Shortcut application tuned to the 2025 TAC (1,552,500 t) but with different constraints on TAC changes. Left most is no constraint, middle is 15% increase and decrease, right-most is 25% decrease and 15% increase.\n\n\n\n\n\nShow code\nlibrary(Slick)\ntargs_lim2515 &lt;- FLmses(targets_lim2515, statistics=statistics,\n               years=2024:2045, metrics=mets, type=\"scgb_d2515\")\n\ntargs_lim &lt;- FLmses(targets_lim15, statistics=statistics,\n               years=2024:2045, metrics=mets, type=\"scgb_d15\")\n\ntargs &lt;- FLmses(targets, statistics=statistics,\n               years=2024:2045, metrics=mets, type=\"scgb_d00\")\nperf &lt;- rbind(performance(targs),performance(targs_lim))\nomperf &lt;- performance(om, statistics=statistics[c(\"C\", \"F\", \"SB\")])\nwritePerformance(perf, file=\"demo/targets.dat.gz\")\nhead(perf)\ntail(perf)\nunique(perf$mp)\n\nsli &lt;- getSlick(perf, omperf, kobeyrs=2034:2042)\nsli@MPs@Label\nsli@MPs@Label &lt;- c(\"0.85\", \"1.0\", \"1.15\", \"1.25\", \"0.85_d15\", \"1.0_d15\", \"1.15_d15\", \"1.25_d15\"    )\nsli@MPs@Description &lt;- sli@MPs@Label\nsli@MPs@Code &lt;- sli@MPs@Label\nApp(slick = sli)\n\n#--Now show contrasts for current TAC limit (1,552,500 t)\n\nperformance(targs_lim2515)\nperf &lt;- rbind(performance(targs),performance(targs_lim),performance(targs_lim2515))\n# ptmp &lt;- perf |&gt; filter(str_detect(mp, \"1552.5\")) \n# unique(ptmp$mp)\nsli &lt;- getSlick(perf, omperf, kobeyrs=2034:2042)\nsli@MPs@Label\nsli@MPs@Label &lt;- c(\"0.85\", \"1.0\", \"1.15\", \"1.25\", \n                   \"0.85_d15\", \"1.0_d15\", \"1.15_d15\", \"1.25_d15\", \n                   \"0.85_d2515\", \"1.0_d2515\", \"1.15_d2515\", \"1.25_d2515\"  )\nsli@MPs@Description &lt;- sli@MPs@Label\nsli@MPs@Code &lt;- sli@MPs@Label\nApp(slick = sli)\n\nsaveRDS(sli, file=\"demo/PayaShortCuts_2.slick\")\n\n\n\n\nOther tools for viewing performance indicators\n\nFurther performance evaluation of the shortcut MPs\nThis section summarizes performance using FLR tools from the mseviz package (Mosqueira (2022)). Average values are computed for specified periods and used in BRP (Biological Reference Point) and tradeoff plots.\n\n\nShow code\nlibrary(mseviz)\nwritePerformance(perf,\"demo/performance.dat.gz\")\nperiods &lt;- list(\n  tuning = 2034:2045,\n  short  = 2025:2027,\n  medium = 2027:2032,\n  long   = 2033:2045\n)\n\n\n\n\nShow code\nperf &lt;- readPerformance(\"demo/performance.dat.gz\") \nperf &lt;- perf |&gt; mutate(data=ifelse((statistic=='F'&data&gt;2),2,data))\n#unique(perf$statistic)\nperf&lt;-  perf %&gt;%\n  mutate(\n    # Extract the part after \"_d\" and before \"_hcr\"\n    d_part = str_extract(mp, \"(?&lt;=_d)[^_]+\"),\n    # Extract the trailing number after the last \"_\"\n    target_num = str_extract(mp, \"[0-9.]+$\"),\n    # Create short name by combining them\n    mp  = paste0(\"d\", d_part, \"_\", \n                 ifelse(target_num==1319.62,\"0.85\",\n                 ifelse(target_num==1552.5,\"1.0\",\n                 ifelse(target_num==1785.37,\"1.15\",\"1.25\"))))\n  )\nperf &lt;- periodsPerformance(perf, periods) \n# perf |&gt; filter(period=='tuning', statistic %in% c(\"SB\",\"C\", \"IACC\"),  mp %in% c(\"d00_1.0\", \"d15_1.0\", \"d2515_1.0\")) |&gt;\n#   ggplot(aes(x=as.factor(mp),y=data,fill=mp)) + geom_boxplot(outlier.shape=NA) + ggthemes::theme_few() +\n#   facet_wrap(.~statistic, scales=\"free_y\") \n\npng(\"images/sc_delta_TAC.png\", width=900, height=800)\nplotBPs(perf |&gt; \n          filter(period=='tuning',  mp %in% c(\n            \"d00_1.0\", \n            \"d15_1.0\", \n            \"d2515_1.0\"\n            )), statistics=c(\n            \"C\",\n            \"IACC\",\n            \"F\",\n            \"PTAClimit\",\n            \"SB\" \n            )) +\n  ggtitle(\"Shortcut MPs for Tuning Period (2034–2045)\") + ggthemes::theme_few(base_size = 16) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_flr() + ylim(c(0,NA)) + xlab(\"MP\")\n  \ndev.off()\n\npng(\"images/sc_targ_TAC.png\", width=900, height=800)\nplotBPs(perf |&gt; \n          filter(period=='tuning',  mp %in% c(\n            \"d00_0.85\", \n            \"d00_1.0\", \n            \"d00_1.15\", \n            \"d00_1.25\" \n            )), statistics=c(\n            \"C\",\n            \"IACC\",\n            \"F\",\n            \"SB\" \n            )) +\n  ggtitle(\"Shortcut MPs for Tuning Period (2034–2045)\") + ggthemes::theme_few(base_size = 16) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_flr() + ylim(c(0,NA)) + xlab(\"MP\")\n  \ndev.off()\n\n\nThe performance of multiple shortcut MPs over the tuning period (2034–2045) were contrasted as an example. Here, each MP was run with catch “targets” at different level relative to the 2025 TAC (1.5525 million t). They ranged from 85% (d00_0.85) to 125% (d00_1.25), with no constraints on interannual TAC changes. As expected, higher catch targets result in greater average catches (mean(C)), but also increased interannual variability (IAC(C)), as well as higher fishing mortality (F) (Figure 7). Conversely, spawning biomass (SB) declines with increasing catch target, suggesting a clear trade-off between yield and stock conservation. While d00_1.0 balances moderate catch with more stable biomass and fishing pressure, higher targets (d00_1.15 and d00_1.25) achieve larger catches at the cost of reduced SB and greater volatility—highlighting the importance of considering both yield and stability objectives when selecting candidate MPs.\nWe then compared performance of shortcut MPs all tuned to the 2025 TAC target (1.5525 million t), but with different constraints on interannual changes in TAC. These included (as in the previous figure) no constraint on TAC changes (d00), a symmetric ±15% constraint (d15), and an asymmetric −25%/+15% constraint (d2515). While mean(C) is similar across all three MPs, the application of TAC constraints notably reduces IAC(C) compared to the unconstrained case (Figure 8). This stability comes with trade-offs—particularly a modest increase in the probability of hitting a predefined TAC floor (P(TAClimit)) for d15 and d2515. SB and F remain broadly similar across scenarios, suggesting that moderate TAC constraints can improve catch stability without severely compromising stock status. Overall, the results highlight the stabilizing benefit of delta-TAC constraints, with d15 offering the most consistent balance of catch, stability, and conservation performance.\nAs an alternative, we show figures that highlight short-term (2025–2027) trade-offs among shortcut MPs based on either different TAC targets (Figure 9) or different TAC change constraints (Figure 10). Increasing the TAC target from 85% to 125% of the 2025 TAC results in expected increases in catch, but also in F and IAC(C), with slight declines in spawning SB. All options show negligible P(TAClimit). In contrast, comparing MPs with the same TAC target (2025 level) but different delta-TAC constraints (Figure 10). While d00_1.0 provides slightly higher short-term catch, it exhibits greater catch variability and a marginally higher risk of triggering the TAC limit compared to d15_1.0 and d2515_1.0. These results suggest that in the short term, applying TAC constraints can enhance stability and reduce the risk of severe TAC cuts, albeit in catch—highlighting a management choice between maximizing short-term yield and reducing volatility.\nWhile the general patterns observed in the short-term persist, the medium-term results show reduced separation across MPs in all performance metrics. For instance, in the TAC target comparison (top panel), differences in F, IAC(C), and SB across catch targets narrow considerably. This suggests that the system has begun to stabilize, with stock status and catch performance converging even under different TAC target levels (Figure 11). Similarly, in the delta-TAC constraint comparison (Figure 12), the three strategies (d00, d15, d2515) yield almost indistinguishable outcomes across all metrics, aside from slightly higher uncertainty in the risk of hitting the TAC limit for d15_1.0. This convergence indicates that the influence of TAC constraints diminishes as the system settles, implying that short-term trade-offs in volatility and yield may be more relevant than medium-term differences when selecting among MPs.\n\n\n\n\n\n\nFigure 7: Boxplots for shortcut MP with target catches set to different multipliers of the 2025 TAC (e.g., 0.85 is 85% of the 2025 TAC (1.5525 million t).\n\n\n\n\n\n\n\n\n\nFigure 8: Boxplots for shortcut MP with target catches set to the 2025 TAC (1.5525 million t) and with different constraints on annual TAC changes.\n\n\n\n\n\n\n\n\n\nFigure 9: “Trade-off plots for short term results from the shortcut method and different targets relative to the 2025 TAC (1.5525 million t).\n\n\n\n\n\n\n\n\n\nFigure 10: “Trade-off plots for short term results from the shortcut method and different constraints on annual TAC changes).\n\n\n\n\n\n\n\n\n\nFigure 11: “Trade-off plots for medium term results from the shortcut method and different targets relative to the 2025 TAC (1.5525 million t).\n\n\n\n\n\n\n\n\n\nFigure 12: “Trade-off plots for medium term results from the shortcut method and different constraints on annual TAC changes).\n\n\n\n\n\nShow code\npng(\"images/sc_delta_TO.png\", width=900, height=450)\nplotTOs(perf |&gt; \n          filter(period=='short',  mp %in% c(\n            \"d00_1.0\", \n            \"d15_1.0\", \n            \"d2515_1.0\"\n            )), x=\"C\",\n        y=c(\n            \"IACC\",\n            \"F\",\n            \"PTAClimit\",\n            \"SB\" \n            )) + \n  ggtitle(\"Shortcut MPs for Short-Term Period (2025–2027)\") + ggthemes::theme_few(base_size = 16) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_flr() + ylim(c(0,NA)) + xlab(\"Catch\")\ndev.off()\n\n\npng(\"images/sc_targs_TO_med.png\", width=900, height=450)\nplotTOs(perf |&gt; \n          filter(period=='medium',  mp %in% c(\n            \"d00_0.85\", \n            \"d00_1.0\", \n            \"d00_1.15\", \n            \"d00_1.25\" \n            )), x=\"C\",\n        y=c(\n            \"IACC\",\n            \"F\",\n            \"PTAClimit\",\n            \"SB\" \n            )) + \n  ggtitle(\"Shortcut MPs for Medium-Term Period (2027–2032)\") + ggthemes::theme_few(base_size = 16) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_flr() + ylim(c(0,NA)) + xlab(\"Catch\")\ndev.off()\n\n\npng(\"images/sc_delta_TO_med.png\", width=900, height=450)\nplotTOs(perf |&gt; \n          filter(period=='medium',  mp %in% c(\n            \"d00_1.0\", \n            \"d15_1.0\", \n            \"d2515_1.0\"\n            )), x=\"C\",\n        y=c(\n            \"IACC\",\n            \"F\",\n            \"PTAClimit\",\n            \"SB\" \n            )) + \n  ggtitle(\"Shortcut MPs for Medium-Term Period (2027–2032)\") + ggthemes::theme_few(base_size = 16) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_flr() + ylim(c(0,NA)) + xlab(\"Catch\")\ndev.off()\n\n\npng(\"images/sc_targs_TO.png\", width=900, height=450)\nplotTOs(perf |&gt; \n          filter(period=='short',  mp %in% c(\n            \"d00_0.85\", \n            \"d00_1.0\", \n            \"d00_1.15\", \n            \"d00_1.25\" \n            )), x=\"C\",\n        y=c(\n            \"IACC\",\n            \"F\",\n            \"PTAClimit\",\n            \"SB\" \n            )) + \n  ggtitle(\"Shortcut MPs for Short-Term Period (2025–2027)\") + ggthemes::theme_few(base_size = 16) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_flr() + ylim(c(0,NA)) + xlab(\"Catch\")\ndev.off()\n\n\n\n\nShortcut sensitivity analysis\nThe shortcut method is designed to simplify the estimation of a stock assessment process. The results shown in the previous section illustrate how this can be reflected based on performance metrics. Here we test the sensitivity of the shortcut method if in fact the random variability (i.e., “noisiness”) in stock size estimates is greater or if there is a bias in stock size estimates.\nThe goal of this section is to see if the constraints affect performance indicators differently than the previous section. We will use the same shortcut method but with added noise and potential bias in the stock size estimates. Results show that the diagnostics are insensitive to the noise and bias in the stock size estimates (Figure 13). Similarly, except for the IAC(C), the performance indicators were unaffected by the banking and borrowing (as part of the implementation error specification; Figure 14). “Banking” TAC, which involves transferring any unused catch to the following year’s catch limit, tended to lower the inter-annual variability in catch whereas “borrowing” (where the catch in excess of the current year’s limit is deducted from the next year’s limit)  increased it.\nFrom this we can conclude that, while incomplete in the context of a final set of candidate MPs, the SC could recommend an interim measure based on the characteristics of different TAC change constraints.\n\n\n\n\n\n\nFigure 13: “Comparison of performance indicators for different”shortcut” MPs all tuned to satisfy the constraint that they result in 60% probability of being in the green zone of the Kobe plot.\n\n\n\n\n\n\n\n\n\nFigure 14: “Comparison of performance indicators for different”banking and borrowing” configurations for MPs all tuned to satisfy the constraint that they result in 60% probability of being in the green zone of the Kobe plot.\n\n\n\n\n\n\nSummary\nThese diagnostics provide visual summaries of CMP performance across time horizons. This addendum presents a full example of how shortcut methods can be configured and evaluated in the jmMSE framework. The use of a buffered harvest control, lognormal deviations, and simplified estimation methods make these examples especially useful for scoping and tuning phases of MSE development. Future iterations could generalize the estimation block or integrate OpenMSE for better interoperability.",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "doc/App_B.html",
    "href": "doc/App_B.html",
    "title": "Appendix B, Agenda",
    "section": "",
    "text": "Welcome and Introduction\nObjective: Set the stage for the focused discussion on Jack Mackerel MSE progress and future directions.\nNote: Briefly introduced the MSE as a tool for sustainable fisheries management, emphasizing its importance given the historical fluctuations in jack mackerel stock and exploitation levels.\nCurrent Status of Jack Mackerel MSE Work\nObjective: Provide a concise update on progress, including advancements in OMs and MP testing.\nNote: MSE development has progressed rapidly. Updates include new data inputs, refinements from the SCW14 benchmark, and expanded uncertainty axes. MPs are under active testing.\nReview of Candidate MPs and Tuning Results\nObjective: Discuss MP structure, indicator choices, and implications of tuning to achieve target performance criteria.\nNote: Focused on empirical MPs using CPUE and acoustic indices. Tuning challenges under high biomass conditions were highlighted.\nRobustness Scenarios and Specification Refinement\nObjective: Finalize the list of scenarios for robustness testing.\nNote: Scenarios included El Niño-like variability, availability shifts, and alternative stock structures. Their role as comparative stress tests was reaffirmed.\nEvaluation Metrics and Visualization Tools\nObjective: Review tools for comparing MP performance.\nNote: Emphasis on visual summaries, including Kobe plots, probability tiles, and trade-off diagrams.\nFeedback from External Experts\nObjective: Integrate external review findings and technical suggestions.\nNote: Dr. Parma’s input emphasized realistic assumptions, consistent reference points, and long-term performance evaluation.\nWrap-up and Next Steps\nObjective: Identify action items and prepare for upcoming reporting deadlines.\nNote: Plans were set for refining MPs, running full simulations, and summarizing results in a format accessible to decision-makers.",
    "crumbs": [
      "Appendices",
      "Agenda"
    ]
  },
  {
    "objectID": "doc/App_D.html",
    "href": "doc/App_D.html",
    "title": "Appendix D. Summary comments from external experts",
    "section": "",
    "text": "Dr. Ana Parma, reviewed the jmMSE framework and its application to evaluating candidate management procedures (CMPs) for jack mackerel. Their comments provide both a validation of the current approach and targeted suggestions for improvement.\n\n0.1 General Observations on the jmMSE Tool\nThe jmMSE package offers a comprehensive and flexible platform for conducting Management Strategy Evaluation (MSE). It includes:\n\nA reference set of Operating Models conditioned to historical data using MCMC.\n\nA suite of visualization tools and performance metrics for comparing CMPs.\n\nAn efficient tuning algorithm that adjusts user-selected MP parameters to achieve a target outcome, such as a probability of being in the green zone.\n\nA variety of robustness tests were developed to address key uncertainties identified for jack mackerel, such as recruitment variability, fleet-specific availability, and stock structure assumptions (one vs. two stocks). Many of these assumptions relate to potential impacts from El Niño-like events. These robustness scenarios were refined during the workshop, and their role was clarified as stress tests—designed not to predict specific mechanisms, but to evaluate the relative performance of CMPs under plausible alternative futures.\n\n\n0.2 Management Procedures Reviewed\nTwo primary classes of empirical MPs were evaluated:\n\nTarget-based MPs: TAC is calculated as a function of a fixed target catch multiplied by an index-driven adjustment factor.\n\nIncremental MPs: TAC is adjusted from the previous period based on indicator trends, resulting in smoother changes over time.\n\nDuring tuning (e.g., to achieve P(Green) = 0.6), both approaches frequently led to increased TACs and eventual stock declines—especially under a high current stock status. This points to the need for additional metrics that reflect long-term sustainability, not just near-term status probabilities.\n\n\n0.3 Technical Recommendations\nSome actionable recommendations were articulated:\n\nIndicator Visualization: Add plots showing the time series of indicators used to drive each HCR.\n\nProjection-End Metrics: Include statistics that summarize stock status and trends at the end of the projection period. This could include P(Green) in the final year or a new trend-based metric.\n\nWeight-at-Age Specification: Enable projections to use mean weights-at-age over a recent period (as done for selectivity), rather than fixing weights from the start year.\n\nConsistency of Reference Points: Ensure that FMSY and SSBMSY reference points used for performance metrics are consistent with the selectivity, weights-at-age, and fleet composition used in projections. This is critical since MPs are tuned relative to these reference points.\n\nObservation Error Realism: Consider adding robustness scenarios with variable selectivity and weight-at-age during projections to better represent realistic observation error in indices—particularly for commercial CPUE.",
    "crumbs": [
      "Appendices",
      "External experts"
    ]
  }
]